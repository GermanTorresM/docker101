{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Series of labs and instructions to introduce you to containers and Docker. Learn to run a container, inspect a container and understand the isolation of processes, create a Dockerfile, build an image from a Dockerfile and understand layers, tag and push images to a registry, scale and update containers, and more. About this workshop \u00b6 This series has an additional presentation. Agenda \u00b6 Lab 0 Pre-work Lab 1 Run your first container Lab 2 Add value with custom images Lab 3 Manage data in containers Pre-requirements \u00b6 For this workshop you must have: Docker CLI, Docker Engine, Docker Registry account, Compatibility \u00b6 This workshop has been tested on the following platforms: Play with Docker , Docker Desktop : v2.2.0.5 Engine : v19.03.8 Compose : v1.25.4 Docker for Linux : Client : v18.09.7 Docker Engine : v19.03.5 Technology Used \u00b6 Docker CLI , Docker Hub . Credits \u00b6 John Zaccone Jason Kennedy Steve Martinelli Remko de Knikker","title":"About the workshop"},{"location":"#introduction","text":"Series of labs and instructions to introduce you to containers and Docker. Learn to run a container, inspect a container and understand the isolation of processes, create a Dockerfile, build an image from a Dockerfile and understand layers, tag and push images to a registry, scale and update containers, and more.","title":"Introduction"},{"location":"#about-this-workshop","text":"This series has an additional presentation.","title":"About this workshop"},{"location":"#agenda","text":"Lab 0 Pre-work Lab 1 Run your first container Lab 2 Add value with custom images Lab 3 Manage data in containers","title":"Agenda"},{"location":"#pre-requirements","text":"For this workshop you must have: Docker CLI, Docker Engine, Docker Registry account,","title":"Pre-requirements"},{"location":"#compatibility","text":"This workshop has been tested on the following platforms: Play with Docker , Docker Desktop : v2.2.0.5 Engine : v19.03.8 Compose : v1.25.4 Docker for Linux : Client : v18.09.7 Docker Engine : v19.03.5","title":"Compatibility"},{"location":"#technology-used","text":"Docker CLI , Docker Hub .","title":"Technology Used"},{"location":"#credits","text":"John Zaccone Jason Kennedy Steve Martinelli Remko de Knikker","title":"Credits"},{"location":"SUMMARY/","text":"Table of contents \u00b6 Introduction Getting Started \u00b6 Pre-work Docker 101 \u00b6 Lab 1 Lab 2 Lab 3 Resources \u00b6 IBM Developer","title":"Table of contents"},{"location":"SUMMARY/#table-of-contents","text":"Introduction","title":"Table of contents"},{"location":"SUMMARY/#getting-started","text":"Pre-work","title":"Getting Started"},{"location":"SUMMARY/#docker-101","text":"Lab 1 Lab 2 Lab 3","title":"Docker 101"},{"location":"SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"lab-0/","text":"Lab 0 - Install Docker \u00b6 Overview \u00b6 In this lab, you will install docker. We will be using docker throughout the rest of the labs. Prerequisites \u00b6 None Use Theia - Cloud IDE (With Docker) \u00b6 Go to https://labs.cognitiveclass.ai , Sign up for CognitiveClass.ai using your IBM id , Login with your IBM Id , Select Theia - Cloud IDE (With Docker) , Select Terminal > New Terminal . docker version Optional: Install Docker Desktop \u00b6 Navigate to Get Docker , Select the option for your operating system or platform: Docker Desktop for Mac, Docker Desktop for Windows, Docker for Linux, On this page you will find the installation for your operating systems. For example, if you are using a Mac, select \"MacOS\", to find the installation for the Mac platform. For Mac, you are re-directed to https://hub.docker.com/editions/community/docker-ce-desktop-mac where you can click the Get Stable edition, Optional: Use play-with-docker \u00b6 If you don't want to install Docker, an alternative is to use Play-With-Docker . Play-With-Docker is a website where you can run terminals directly from your browser that have Docker installed. All of the labs for this course can be run on Play-With-Docker, though we recommend installing docker locally on your host, so that you can continue your docker journey when this course has completed. To use Play-With-Docker, navigate to Play-With-Docker in your browser. Summary \u00b6 By installing Docker, or alternatively, familiarizing yourself with Play with Docker , you are ready to complete the remaining labs in this course.","title":"Getting Started"},{"location":"lab-0/#lab-0-install-docker","text":"","title":"Lab 0 - Install Docker"},{"location":"lab-0/#overview","text":"In this lab, you will install docker. We will be using docker throughout the rest of the labs.","title":"Overview"},{"location":"lab-0/#prerequisites","text":"None","title":"Prerequisites"},{"location":"lab-0/#use-theia-cloud-ide-with-docker","text":"Go to https://labs.cognitiveclass.ai , Sign up for CognitiveClass.ai using your IBM id , Login with your IBM Id , Select Theia - Cloud IDE (With Docker) , Select Terminal > New Terminal . docker version","title":"Use Theia - Cloud IDE (With Docker)"},{"location":"lab-0/#optional-install-docker-desktop","text":"Navigate to Get Docker , Select the option for your operating system or platform: Docker Desktop for Mac, Docker Desktop for Windows, Docker for Linux, On this page you will find the installation for your operating systems. For example, if you are using a Mac, select \"MacOS\", to find the installation for the Mac platform. For Mac, you are re-directed to https://hub.docker.com/editions/community/docker-ce-desktop-mac where you can click the Get Stable edition,","title":"Optional: Install Docker Desktop"},{"location":"lab-0/#optional-use-play-with-docker","text":"If you don't want to install Docker, an alternative is to use Play-With-Docker . Play-With-Docker is a website where you can run terminals directly from your browser that have Docker installed. All of the labs for this course can be run on Play-With-Docker, though we recommend installing docker locally on your host, so that you can continue your docker journey when this course has completed. To use Play-With-Docker, navigate to Play-With-Docker in your browser.","title":"Optional: Use play-with-docker"},{"location":"lab-0/#summary","text":"By installing Docker, or alternatively, familiarizing yourself with Play with Docker , you are ready to complete the remaining labs in this course.","title":"Summary"},{"location":"lab-1/","text":"Lab 1 - Running Your First Container \u00b6 Overview \u00b6 In this lab, you will run your first Docker container. Containers are just a process (or a group of processes) running in isolation. Isolation is achieved via linux namespaces, control groups (cgroups), seccomp and SELinux. Note that linux namespaces and control groups are built into the linux kernel! Other than the linux kernel itself, there is nothing special about containers. What makes containers useful is the tooling that surrounds it. For these labs, we will be using Docker, which has been a widely adopted tool for using containers to build applications. Docker provides developers and operators with a friendly interface to build, ship and run containers on any environment with a Docker engine. Because Docker client requires a Docker engine, an alternative is to use Podman , which is a deamonless container engine to develop, manage and run OCI containers and is able to run containers as root or in rootless mode. For those reasons, we recommend Podman but because of adoption, this lab still uses Docker. The first part of this lab, we will run our first container, and learn how to inspect it. We will be able to witness the namespace isolation that we acquire from the linux kernel. After we run our first container, we will dive into other uses of containers. You can find many examples of these on the Docker Store, and we will run several different types of containers on the same host. This will allow us to see the benefit of isolation- where we can run multiple containers on the same host without conflicts. We will be using a few Docker commands in this lab. For full documentation on available commands check out the official documentation . Prerequisites \u00b6 Completed Lab 0: You must have access to a docker client, either on localhost, use a terminal from Theia - Cloud IDE at https://labs.cognitiveclass.ai/tools/theiadocker or be using Play with Docker for example. Get Started \u00b6 Run docker -h , $ docker -h Flag shorthand -h has been deprecated, please use --help Usage: docker [OPTIONS] COMMAND A self-sufficient runtime for containers ... Management Commands: builder Manage builds config Manage Docker configs container Manage containers engine Manage the docker engine image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker trust Manage trust on Docker images volume Manage volumes The Docker command line can be used to manage several features of the Docker Engine. In this lab, we will mainly focus on the container command. If podman is installed, you can run the alternative command for comparison. sudo podman -h You can additionally review the version of your Docker installation, docker version Client: Version: 19 .03.6 ... Server: Docker Engine - Community Engine Version: 19 .03.5 ... You note that Docker installs both a Client and a Server: Docker Engine . For instance, if you run the same command for podman, you will see only a CLI version, because podman runs daemonless and relies on an OCI compliant container runtime (runc, crun, runv etc) to interface with the OS to create the running containers. sudo podman version --events-backend = none Version: 2 .1.1 API Version: 2 .0.0 Go Version: go1.15.2 Built: Thu Jan 1 00 :00:00 1970 OS/Arch: linux/amd64 Step 1: Run your first container \u00b6 We are going to use the Docker CLI to run our first container. Open a terminal on your local computer Run docker container run -t ubuntu top Use the docker container run command to run a container with the ubuntu image using the top command. The -t flags allocate a pseudo-TTY which we need for the top to work correctly. $ docker container run -it ubuntu top Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu aafe6b5e13de: Pull complete 0a2b43a72660: Pull complete 18bdd1e546d2: Pull complete 8198342c3e05: Pull complete f56970a44fd4: Pull complete Digest: sha256:f3a61450ae43896c4332bda5e78b453f4a93179045f20c8181043b26b5e79028 Status: Downloaded newer image for ubuntu:latest The docker run command will result first in a docker pull to download the ubuntu image onto your host. Once it is downloaded, it will start the container. The output for the running container should look like this: top - 20 :32:46 up 3 days, 17 :40, 0 users, load average: 0 .00, 0 .01, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .0 us, 0 .1 sy, 0 .0 ni, 99 .9 id, 0 .0 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046768 total, 173308 free, 117248 used, 1756212 buff/cache KiB Swap: 1048572 total, 1048572 free, 0 used. 1548356 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36636 3072 2640 R 0 .3 0 .2 0 :00.04 top top is a linux utility that prints the processes on a system and orders them by resource consumption. Notice that there is only a single process in this output: it is the top process itself. We don't see other processes from our host in this list because of the PID namespace isolation. Containers use linux namespaces to provide isolation of system resources from other containers or the host. The PID namespace provides isolation for process IDs. If you run top while inside the container, you will notice that it shows the processes within the PID namespace of the container, which is much different than what you can see if you ran top on the host. Even though we are using the ubuntu image, it is important to note that our container does not have its own kernel. Its uses the kernel of the host and the ubuntu image is used only to provide the file system and tools available on an ubuntu system. Inspect the container with docker container exec The docker container exec command is a way to \"enter\" a running container's namespaces with a new process. Open a new terminal. On cognitiveclass.ai, select Terminal > New Terminal . Using play-with-docker.com, to open a new terminal connected to node1, click \"Add New Instance\" on the lefthand side, then ssh from node2 into node1 using the IP that is listed by 'node1 '. For example: [node2] (local) root@192.168.0.17 ~ $ ssh 192 .168.0.18 [node1] (local) root@192.168.0.18 ~ $ In the new terminal, use the docker container ls command to get the ID of the running container you just created. $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b3ad2a23fab3 ubuntu \"top\" 29 minutes ago Up 29 minutes goofy_nobel Then use that id to run bash inside that container using the docker container exec command. Since we are using bash and want to interact with this container from our terminal, use -it flags to run using interactive mode while allocating a psuedo-terminal. $ docker container exec -it b3ad2a23fab3 bash root@b3ad2a23fab3:/# And Voila! We just used the docker container exec command to \"enter\" our container's namespaces with our bash process. Using docker container exec with bash is a common pattern to inspect a docker container. Notice the change in the prefix of your terminal. e.g. root@b3ad2a23fab3:/ . This is an indication that we are running bash \"inside\" of our container. Note : This is not the same as ssh'ing into a separate host or a VM. We don't need an ssh server to connect with a bash process. Remember that containers use kernel-level features to achieve isolation and that containers run on top of the kernel. Our container is just a group of processes running in isolation on the same host, and we can use docker container exec to enter that isolation with the bash process. After running docker container exec , the group of processes running in isolation (i.e. our container) include top and bash . From the same termina, run ps -ef to inspect the running processes. root@b3ad2a23fab3:/# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 20:34 ? 00:00:00 top root 17 0 0 21:06 ? 00:00:00 bash root 27 17 0 21:14 ? 00:00:00 ps -ef You should see only the top process, bash process and our ps process. For comparison, exit the container, and run ps -ef or top on the host. These commands will work on linux or mac. For windows, you can inspect the running processes using tasklist . root@b3ad2a23fab3:/# exit exit $ ps -ef # Lots of processes! Technical Deep Dive PID is just one of the linux namespaces that provides containers with isolation to system resources. Other linux namespaces include: - MNT - Mount and unmount directories without affecting other namespaces - NET - Containers have their own network stack - IPC - Isolated interprocess communication mechanisms such as message queues. - User - Isolated view of users on the system - UTC - Set hostname and domain name per container These namespaces together provide the isolation for containers that allow them to run together securely and without conflict with other containers running on the same system. Next, we will demonstrate different uses of containers. and the benefit of isolation as we run multiple containers on the same host. Note : Namespaces are a feature of the linux kernel. But Docker allows you to run containers on Windows and Mac... how does that work? The secret is that embedded in the Docker product or Docker engine is a linux subsystem. Docker open-sourced this linux subsystem to a new project: LinuxKit . Being able to run containers on many different platforms is one advantage of using the Docker tooling with containers. In addition to running linux containers on Windows using a linux subsystem, native Windows containers are now possible due the creation of container primitives on the Windows OS. Native Windows containers can be run on Windows 10 or Windows Server 2016 or newer. Note : if you run this exercise in a containerized terminal and execute the ps -ef command in the terminal, e.g. in https://labs.cognitiveclass.ai , you will still see a limited set of processes after exiting the exec command. You can try to run the ps -ef command in a terminal on your local machine to see all processes. Clean up the container running the top processes by typing: <ctrl>-c , list all containers and remove the containers by their id. docker ps -a docker rm <CONTAINER ID> Step 2: Run Multiple Containers \u00b6 Explore the Docker Hub The Docker Hub is the public central registry for Docker images, which contains community and official images. When searching for images you will find filters for \"Docker Certified\", \"Verified Publisher\" and \"Official Images\" images. Select the \"Docker Certified\" filter, to find images that are deemed enterprise-ready and are tested with Docker Enterprise Edition product. It is important to avoid using unverified content from the Docker Store when developing your own images that are intended to be deployed into the production environment. These unverified images may contain security vulnerabilities or possibly even malicious software. In Step 2 of this lab, we will start a couple of containers using some verified images from the Docker Hub: nginx web server, and mongo database. Run an Nginx server Let's run a container using the official Nginx image from the Docker Hub. $ docker container run --detach --publish 8080 :80 --name nginx nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 36a46ebd5019: Pull complete 57168433389f: Pull complete 332ec8285c50: Pull complete Digest: sha256:c15f1fb8fd55c60c72f940a76da76a5fccce2fefa0dd9b17967b9e40b0355316 Status: Downloaded newer image for nginx:latest 5e1bf0e6b926bd73a66f98b3cbe23d04189c16a43d55dd46b8486359f6fdf048 We are using a couple of new flags here. The --detach flag will run this container in the background. The publish flag publishes port 80 in the container (the default port for nginx), via port 8080 on our host. Remember that the NET namespace gives processes of the container their own network stack. The --publish flag is a feature that allows us to expose networking through the container onto the host. How do you know port 80 is the default port for nginx? Because it is listed in the documentation on the Docker Hub. In general, the documentation for the verified images is very good, and you will want to refer to them when running containers using those images. We are also specifying the --name flag, which names the container. Every container has a name, if you don't specify one, Docker will randomly assign one for you. Specifying your own name makes it easier to run subsequent commands on your container since you can reference the name instead of the id of the container. For example: docker container inspect nginx instead of docker container inspect 5e1 . Since this is the first time you are running the nginx container, it will pull down the nginx image from the Docker Store. Subsequent containers created from the Nginx image will use the existing image located on your host. Nginx is a lightweight web server. You can access it on port 8080 on your localhost. Access the nginx server on localhost:8080 . curl localhost:8080 will return the HTML home page of Nginx, <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em ; margin: 0 auto ; font-family: Tahoma, Verdana, Arial, sans-serif ; } </style> </head> <body> <h1>Welcome to nginx!</h1> If you are using play-with-docker, look for the 8080 link near the top of the page, or if you run a Docker client with access to a local browser, Run a mongo DB server Now, run a mongoDB server. We will use the official mongoDB image from the Docker Hub. Instead of using the latest tag (which is the default if no tag is specified), we will use a specific version of the mongo image: 4.4. $ docker container run --detach --publish 8081 :27017 --name mongo mongo:4.4 Unable to find image mongo:4.4 locally 4 .4: Pulling from library/mongo d13d02fa248d: Already exists bc8e2652ce92: Pull complete 3cc856886986: Pull complete c319e9ec4517: Pull complete b4cbf8808f94: Pull complete cb98a53e6676: Pull complete f0485050cd8a: Pull complete ac36cdc414b3: Pull complete 61814e3c487b: Pull complete 523a9f1da6b9: Pull complete 3b4beaef77a2: Pull complete Digest: sha256:d13c897516e497e898c229e2467f4953314b63e48d4990d3215d876ef9d1fc7c Status: Downloaded newer image for mongo:4.4 d8f614a4969fb1229f538e171850512f10f490cb1a96fca27e4aa89ac082eba5 Again, since this is the first time we are running a mongo container, we will pull down the mongo image from the Docker Store. We are using the --publish flag to expose the 27017 mongo port on our host. We have to use a port other than 8080 for the host mapping, since that port is already exposed on our host. Again refer to the official docs on the Docker Hub to get more details about using the mongo image. Access localhost:8081 to see some output from mongo. curl localhost:8081 which will return a warning from MongoDB, It looks like you are trying to access MongoDB over HTTP on the native driver port. If you are using play-with-docker, look for the 8080 link near the top of the page. Check your running containers with docker container ls $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6777df89fea nginx \"nginx -g 'daemon ...\" Less than a second ago Up 2 seconds 0 .0.0.0:8080->80/tcp nginx ead80a0db505 mongo \"docker-entrypoint...\" 17 seconds ago Up 19 seconds 0 .0.0.0:8081->27017/tcp mongo af549dccd5cf ubuntu \"top\" 5 minutes ago Up 5 minutes priceless_kepler You should see that you have an Nginx web server container, and a MongoDB container running on your host. Note that we have not configured these containers to talk to each other. You can see the \"nginx\" and \"mongo\" names that we gave to our containers, and the random name (in my case \"priceless_kepler\") that was generated for the ubuntu container. You can also see that the port mappings that we specified with the --publish flag. For more details information on these running containers you can use the docker container inspect [container id command. One thing you might notice is that the mongo container is running the docker-entrypoint command. This is the name of the executable that is run when the container is started. The mongo image requires some prior configuration before kicking off the DB process. You can see exactly what the script does by looking at it on github . Typically, you can find the link to the github source from the image description page on the Docker Store website. Containers are self-contained and isolated, which means we can avoid potential conflicts between containers with different system or runtime dependencies. For example: deploying an app that uses Java 7 and another app that uses Java 8 on the same host. Or running multiple nginx containers that all have port 80 as their default listening ports (if exposing on the host using the --publish flag, the ports selected for the host will need to be unique). Isolation benefits are possible because of Linux Namespaces. Note : You didn't have to install anything on your host (other than Docker) to run these processes! Each container includes the dependencies that it needs within the container, so you don't need to install anything on your host directly. Running multiple containers on the same host gives us the ability to fully utilize the resources (cpu, memory, etc) available on single host. This can result in huge cost savings for an enterprise. While running images directly from the Docker Hub can be useful at times, it is more useful to create custom images, and refer to official images as the starting point for these images. We will dive into building our own custom images in Lab 2. Step 3: Clean Up \u00b6 Completing this lab results in a bunch of running containers on your host. Let's clean these up. First get a list of the containers running using docker container ls . $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6777df89fea nginx \"nginx -g 'daemon ...\" 3 minutes ago Up 3 minutes 0 .0.0.0:8080->80/tcp nginx ead80a0db505 mongo \"docker-entrypoint...\" 3 minutes ago Up 3 minutes 0 .0.0.0:8081->27017/tcp mongo af549dccd5cf ubuntu \"top\" 8 minutes ago Up 8 minutes priceless_kepler Next, run docker container stop [container id] for each container in the list. You can also use the names of the containers that you specified before. $ docker container stop d67 ead af5 d67 ead af5 Note : You only have to reference enough digits of the ID to be unique. Three digits is almost always enough. Remove the stopped containers docker system prune is a really handy command to clean up your system. It will remove any stopped containers, unused volumes and networks, and dangling images. $ docker system prune WARNING! This will remove: - all stopped containers - all volumes not used by at least one container - all networks not used by at least one container - all dangling images Are you sure you want to continue ? [ y/N ] y Deleted Containers: 7872fd96ea4695795c41150a06067d605f69702dbcb9ce49492c9029f0e1b44b 60abd5ee65b1e2732ddc02b971a86e22de1c1c446dab165462a08b037ef7835c 31617fdd8e5f584c51ce182757e24a1c9620257027665c20be75aa3ab6591740 Total reclaimed space: 12B Summary \u00b6 In this lab, you created your first Ubuntu, Nginx and MongoDB containers. Key Takeaways Containers are composed of linux namespaces and control groups that provide isolation from other containers and the host. Because of the isolation properties of containers, you can schedule many containers on a single host without worrying about conflicting dependencies. This makes it easier to run multiple containers on a single host: fully utilizing resources allocated to that host, and ultimately saving some money on server costs. Avoid using unverified content from the Docker Store when developing your own images because these images may contain security vulnerabilities or possibly even malicious software. Containers include everything they need to run the processes within them, so there is no need to install additional dependencies directly on your host.","title":"Lab 1. Running Your First Container"},{"location":"lab-1/#lab-1-running-your-first-container","text":"","title":"Lab 1 - Running Your First Container"},{"location":"lab-1/#overview","text":"In this lab, you will run your first Docker container. Containers are just a process (or a group of processes) running in isolation. Isolation is achieved via linux namespaces, control groups (cgroups), seccomp and SELinux. Note that linux namespaces and control groups are built into the linux kernel! Other than the linux kernel itself, there is nothing special about containers. What makes containers useful is the tooling that surrounds it. For these labs, we will be using Docker, which has been a widely adopted tool for using containers to build applications. Docker provides developers and operators with a friendly interface to build, ship and run containers on any environment with a Docker engine. Because Docker client requires a Docker engine, an alternative is to use Podman , which is a deamonless container engine to develop, manage and run OCI containers and is able to run containers as root or in rootless mode. For those reasons, we recommend Podman but because of adoption, this lab still uses Docker. The first part of this lab, we will run our first container, and learn how to inspect it. We will be able to witness the namespace isolation that we acquire from the linux kernel. After we run our first container, we will dive into other uses of containers. You can find many examples of these on the Docker Store, and we will run several different types of containers on the same host. This will allow us to see the benefit of isolation- where we can run multiple containers on the same host without conflicts. We will be using a few Docker commands in this lab. For full documentation on available commands check out the official documentation .","title":"Overview"},{"location":"lab-1/#prerequisites","text":"Completed Lab 0: You must have access to a docker client, either on localhost, use a terminal from Theia - Cloud IDE at https://labs.cognitiveclass.ai/tools/theiadocker or be using Play with Docker for example.","title":"Prerequisites"},{"location":"lab-1/#get-started","text":"Run docker -h , $ docker -h Flag shorthand -h has been deprecated, please use --help Usage: docker [OPTIONS] COMMAND A self-sufficient runtime for containers ... Management Commands: builder Manage builds config Manage Docker configs container Manage containers engine Manage the docker engine image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker trust Manage trust on Docker images volume Manage volumes The Docker command line can be used to manage several features of the Docker Engine. In this lab, we will mainly focus on the container command. If podman is installed, you can run the alternative command for comparison. sudo podman -h You can additionally review the version of your Docker installation, docker version Client: Version: 19 .03.6 ... Server: Docker Engine - Community Engine Version: 19 .03.5 ... You note that Docker installs both a Client and a Server: Docker Engine . For instance, if you run the same command for podman, you will see only a CLI version, because podman runs daemonless and relies on an OCI compliant container runtime (runc, crun, runv etc) to interface with the OS to create the running containers. sudo podman version --events-backend = none Version: 2 .1.1 API Version: 2 .0.0 Go Version: go1.15.2 Built: Thu Jan 1 00 :00:00 1970 OS/Arch: linux/amd64","title":"Get Started"},{"location":"lab-1/#step-1-run-your-first-container","text":"We are going to use the Docker CLI to run our first container. Open a terminal on your local computer Run docker container run -t ubuntu top Use the docker container run command to run a container with the ubuntu image using the top command. The -t flags allocate a pseudo-TTY which we need for the top to work correctly. $ docker container run -it ubuntu top Unable to find image 'ubuntu:latest' locally latest: Pulling from library/ubuntu aafe6b5e13de: Pull complete 0a2b43a72660: Pull complete 18bdd1e546d2: Pull complete 8198342c3e05: Pull complete f56970a44fd4: Pull complete Digest: sha256:f3a61450ae43896c4332bda5e78b453f4a93179045f20c8181043b26b5e79028 Status: Downloaded newer image for ubuntu:latest The docker run command will result first in a docker pull to download the ubuntu image onto your host. Once it is downloaded, it will start the container. The output for the running container should look like this: top - 20 :32:46 up 3 days, 17 :40, 0 users, load average: 0 .00, 0 .01, 0 .00 Tasks: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie %Cpu ( s ) : 0 .0 us, 0 .1 sy, 0 .0 ni, 99 .9 id, 0 .0 wa, 0 .0 hi, 0 .0 si, 0 .0 st KiB Mem : 2046768 total, 173308 free, 117248 used, 1756212 buff/cache KiB Swap: 1048572 total, 1048572 free, 0 used. 1548356 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 36636 3072 2640 R 0 .3 0 .2 0 :00.04 top top is a linux utility that prints the processes on a system and orders them by resource consumption. Notice that there is only a single process in this output: it is the top process itself. We don't see other processes from our host in this list because of the PID namespace isolation. Containers use linux namespaces to provide isolation of system resources from other containers or the host. The PID namespace provides isolation for process IDs. If you run top while inside the container, you will notice that it shows the processes within the PID namespace of the container, which is much different than what you can see if you ran top on the host. Even though we are using the ubuntu image, it is important to note that our container does not have its own kernel. Its uses the kernel of the host and the ubuntu image is used only to provide the file system and tools available on an ubuntu system. Inspect the container with docker container exec The docker container exec command is a way to \"enter\" a running container's namespaces with a new process. Open a new terminal. On cognitiveclass.ai, select Terminal > New Terminal . Using play-with-docker.com, to open a new terminal connected to node1, click \"Add New Instance\" on the lefthand side, then ssh from node2 into node1 using the IP that is listed by 'node1 '. For example: [node2] (local) root@192.168.0.17 ~ $ ssh 192 .168.0.18 [node1] (local) root@192.168.0.18 ~ $ In the new terminal, use the docker container ls command to get the ID of the running container you just created. $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b3ad2a23fab3 ubuntu \"top\" 29 minutes ago Up 29 minutes goofy_nobel Then use that id to run bash inside that container using the docker container exec command. Since we are using bash and want to interact with this container from our terminal, use -it flags to run using interactive mode while allocating a psuedo-terminal. $ docker container exec -it b3ad2a23fab3 bash root@b3ad2a23fab3:/# And Voila! We just used the docker container exec command to \"enter\" our container's namespaces with our bash process. Using docker container exec with bash is a common pattern to inspect a docker container. Notice the change in the prefix of your terminal. e.g. root@b3ad2a23fab3:/ . This is an indication that we are running bash \"inside\" of our container. Note : This is not the same as ssh'ing into a separate host or a VM. We don't need an ssh server to connect with a bash process. Remember that containers use kernel-level features to achieve isolation and that containers run on top of the kernel. Our container is just a group of processes running in isolation on the same host, and we can use docker container exec to enter that isolation with the bash process. After running docker container exec , the group of processes running in isolation (i.e. our container) include top and bash . From the same termina, run ps -ef to inspect the running processes. root@b3ad2a23fab3:/# ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 20:34 ? 00:00:00 top root 17 0 0 21:06 ? 00:00:00 bash root 27 17 0 21:14 ? 00:00:00 ps -ef You should see only the top process, bash process and our ps process. For comparison, exit the container, and run ps -ef or top on the host. These commands will work on linux or mac. For windows, you can inspect the running processes using tasklist . root@b3ad2a23fab3:/# exit exit $ ps -ef # Lots of processes! Technical Deep Dive PID is just one of the linux namespaces that provides containers with isolation to system resources. Other linux namespaces include: - MNT - Mount and unmount directories without affecting other namespaces - NET - Containers have their own network stack - IPC - Isolated interprocess communication mechanisms such as message queues. - User - Isolated view of users on the system - UTC - Set hostname and domain name per container These namespaces together provide the isolation for containers that allow them to run together securely and without conflict with other containers running on the same system. Next, we will demonstrate different uses of containers. and the benefit of isolation as we run multiple containers on the same host. Note : Namespaces are a feature of the linux kernel. But Docker allows you to run containers on Windows and Mac... how does that work? The secret is that embedded in the Docker product or Docker engine is a linux subsystem. Docker open-sourced this linux subsystem to a new project: LinuxKit . Being able to run containers on many different platforms is one advantage of using the Docker tooling with containers. In addition to running linux containers on Windows using a linux subsystem, native Windows containers are now possible due the creation of container primitives on the Windows OS. Native Windows containers can be run on Windows 10 or Windows Server 2016 or newer. Note : if you run this exercise in a containerized terminal and execute the ps -ef command in the terminal, e.g. in https://labs.cognitiveclass.ai , you will still see a limited set of processes after exiting the exec command. You can try to run the ps -ef command in a terminal on your local machine to see all processes. Clean up the container running the top processes by typing: <ctrl>-c , list all containers and remove the containers by their id. docker ps -a docker rm <CONTAINER ID>","title":"Step 1: Run your first container"},{"location":"lab-1/#step-2-run-multiple-containers","text":"Explore the Docker Hub The Docker Hub is the public central registry for Docker images, which contains community and official images. When searching for images you will find filters for \"Docker Certified\", \"Verified Publisher\" and \"Official Images\" images. Select the \"Docker Certified\" filter, to find images that are deemed enterprise-ready and are tested with Docker Enterprise Edition product. It is important to avoid using unverified content from the Docker Store when developing your own images that are intended to be deployed into the production environment. These unverified images may contain security vulnerabilities or possibly even malicious software. In Step 2 of this lab, we will start a couple of containers using some verified images from the Docker Hub: nginx web server, and mongo database. Run an Nginx server Let's run a container using the official Nginx image from the Docker Hub. $ docker container run --detach --publish 8080 :80 --name nginx nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 36a46ebd5019: Pull complete 57168433389f: Pull complete 332ec8285c50: Pull complete Digest: sha256:c15f1fb8fd55c60c72f940a76da76a5fccce2fefa0dd9b17967b9e40b0355316 Status: Downloaded newer image for nginx:latest 5e1bf0e6b926bd73a66f98b3cbe23d04189c16a43d55dd46b8486359f6fdf048 We are using a couple of new flags here. The --detach flag will run this container in the background. The publish flag publishes port 80 in the container (the default port for nginx), via port 8080 on our host. Remember that the NET namespace gives processes of the container their own network stack. The --publish flag is a feature that allows us to expose networking through the container onto the host. How do you know port 80 is the default port for nginx? Because it is listed in the documentation on the Docker Hub. In general, the documentation for the verified images is very good, and you will want to refer to them when running containers using those images. We are also specifying the --name flag, which names the container. Every container has a name, if you don't specify one, Docker will randomly assign one for you. Specifying your own name makes it easier to run subsequent commands on your container since you can reference the name instead of the id of the container. For example: docker container inspect nginx instead of docker container inspect 5e1 . Since this is the first time you are running the nginx container, it will pull down the nginx image from the Docker Store. Subsequent containers created from the Nginx image will use the existing image located on your host. Nginx is a lightweight web server. You can access it on port 8080 on your localhost. Access the nginx server on localhost:8080 . curl localhost:8080 will return the HTML home page of Nginx, <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em ; margin: 0 auto ; font-family: Tahoma, Verdana, Arial, sans-serif ; } </style> </head> <body> <h1>Welcome to nginx!</h1> If you are using play-with-docker, look for the 8080 link near the top of the page, or if you run a Docker client with access to a local browser, Run a mongo DB server Now, run a mongoDB server. We will use the official mongoDB image from the Docker Hub. Instead of using the latest tag (which is the default if no tag is specified), we will use a specific version of the mongo image: 4.4. $ docker container run --detach --publish 8081 :27017 --name mongo mongo:4.4 Unable to find image mongo:4.4 locally 4 .4: Pulling from library/mongo d13d02fa248d: Already exists bc8e2652ce92: Pull complete 3cc856886986: Pull complete c319e9ec4517: Pull complete b4cbf8808f94: Pull complete cb98a53e6676: Pull complete f0485050cd8a: Pull complete ac36cdc414b3: Pull complete 61814e3c487b: Pull complete 523a9f1da6b9: Pull complete 3b4beaef77a2: Pull complete Digest: sha256:d13c897516e497e898c229e2467f4953314b63e48d4990d3215d876ef9d1fc7c Status: Downloaded newer image for mongo:4.4 d8f614a4969fb1229f538e171850512f10f490cb1a96fca27e4aa89ac082eba5 Again, since this is the first time we are running a mongo container, we will pull down the mongo image from the Docker Store. We are using the --publish flag to expose the 27017 mongo port on our host. We have to use a port other than 8080 for the host mapping, since that port is already exposed on our host. Again refer to the official docs on the Docker Hub to get more details about using the mongo image. Access localhost:8081 to see some output from mongo. curl localhost:8081 which will return a warning from MongoDB, It looks like you are trying to access MongoDB over HTTP on the native driver port. If you are using play-with-docker, look for the 8080 link near the top of the page. Check your running containers with docker container ls $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6777df89fea nginx \"nginx -g 'daemon ...\" Less than a second ago Up 2 seconds 0 .0.0.0:8080->80/tcp nginx ead80a0db505 mongo \"docker-entrypoint...\" 17 seconds ago Up 19 seconds 0 .0.0.0:8081->27017/tcp mongo af549dccd5cf ubuntu \"top\" 5 minutes ago Up 5 minutes priceless_kepler You should see that you have an Nginx web server container, and a MongoDB container running on your host. Note that we have not configured these containers to talk to each other. You can see the \"nginx\" and \"mongo\" names that we gave to our containers, and the random name (in my case \"priceless_kepler\") that was generated for the ubuntu container. You can also see that the port mappings that we specified with the --publish flag. For more details information on these running containers you can use the docker container inspect [container id command. One thing you might notice is that the mongo container is running the docker-entrypoint command. This is the name of the executable that is run when the container is started. The mongo image requires some prior configuration before kicking off the DB process. You can see exactly what the script does by looking at it on github . Typically, you can find the link to the github source from the image description page on the Docker Store website. Containers are self-contained and isolated, which means we can avoid potential conflicts between containers with different system or runtime dependencies. For example: deploying an app that uses Java 7 and another app that uses Java 8 on the same host. Or running multiple nginx containers that all have port 80 as their default listening ports (if exposing on the host using the --publish flag, the ports selected for the host will need to be unique). Isolation benefits are possible because of Linux Namespaces. Note : You didn't have to install anything on your host (other than Docker) to run these processes! Each container includes the dependencies that it needs within the container, so you don't need to install anything on your host directly. Running multiple containers on the same host gives us the ability to fully utilize the resources (cpu, memory, etc) available on single host. This can result in huge cost savings for an enterprise. While running images directly from the Docker Hub can be useful at times, it is more useful to create custom images, and refer to official images as the starting point for these images. We will dive into building our own custom images in Lab 2.","title":"Step 2: Run Multiple Containers"},{"location":"lab-1/#step-3-clean-up","text":"Completing this lab results in a bunch of running containers on your host. Let's clean these up. First get a list of the containers running using docker container ls . $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d6777df89fea nginx \"nginx -g 'daemon ...\" 3 minutes ago Up 3 minutes 0 .0.0.0:8080->80/tcp nginx ead80a0db505 mongo \"docker-entrypoint...\" 3 minutes ago Up 3 minutes 0 .0.0.0:8081->27017/tcp mongo af549dccd5cf ubuntu \"top\" 8 minutes ago Up 8 minutes priceless_kepler Next, run docker container stop [container id] for each container in the list. You can also use the names of the containers that you specified before. $ docker container stop d67 ead af5 d67 ead af5 Note : You only have to reference enough digits of the ID to be unique. Three digits is almost always enough. Remove the stopped containers docker system prune is a really handy command to clean up your system. It will remove any stopped containers, unused volumes and networks, and dangling images. $ docker system prune WARNING! This will remove: - all stopped containers - all volumes not used by at least one container - all networks not used by at least one container - all dangling images Are you sure you want to continue ? [ y/N ] y Deleted Containers: 7872fd96ea4695795c41150a06067d605f69702dbcb9ce49492c9029f0e1b44b 60abd5ee65b1e2732ddc02b971a86e22de1c1c446dab165462a08b037ef7835c 31617fdd8e5f584c51ce182757e24a1c9620257027665c20be75aa3ab6591740 Total reclaimed space: 12B","title":"Step 3: Clean Up"},{"location":"lab-1/#summary","text":"In this lab, you created your first Ubuntu, Nginx and MongoDB containers. Key Takeaways Containers are composed of linux namespaces and control groups that provide isolation from other containers and the host. Because of the isolation properties of containers, you can schedule many containers on a single host without worrying about conflicting dependencies. This makes it easier to run multiple containers on a single host: fully utilizing resources allocated to that host, and ultimately saving some money on server costs. Avoid using unverified content from the Docker Store when developing your own images because these images may contain security vulnerabilities or possibly even malicious software. Containers include everything they need to run the processes within them, so there is no need to install additional dependencies directly on your host.","title":"Summary"},{"location":"lab-2/","text":"Lab 2 - Adding Value with Custom Docker Images \u00b6 Overview \u00b6 In this lab, we build on our knowledge from lab 1 where we used Docker commands to run containers. We will create a custom Docker Image built from a Dockerfile. Once we build the image, we will push it to a central registry where it can be pulled to be deployed on other environments. Also, we will briefly describe image layers, and how Docker incorporates \"copy-on-write\" and the union file system to efficiently store images and run containers. We will be using a few Docker commands in this lab. For full documentation on available commands check out the official documentation . Prerequisites \u00b6 Completed Lab 0: You must have access to a docker client, either on localhost, use a terminal from Theia - Cloud IDE at https://labs.cognitiveclass.ai/tools/theiadocker or be using Play with Docker for example. Step 1: Create a python app (without using Docker) \u00b6 Run the following command to create a file named app.py with a simple python program. (copy-paste the entire code block) echo 'from flask import Flask app = Flask(__name__) @app.route(\"/\") def hello(): return \"hello world!\" if __name__ == \"__main__\": app.run(host=\"0.0.0.0\")' > app.py This is a simple python app that uses flask to expose a http web server on port 5000 (5000 is the default port for flask). Don't worry if you are not too familiar with python or flask, these concepts can be applied to an application written in any language. Optional: If you have python and pip installed, you can run this app locally. If not, move on to the next step. $ python3 --version Python 3.6.9 $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) $ pip3 install flask Collecting flask Downloading https://files.pythonhosted.org/packages/f2/28/2a03252dfb9ebf377f40fba6a7841b47083260bf8bd8e737b0c6952df83f/Flask-1.1.2-py2.py3-none-any.whl (94kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 6.2MB/s Collecting Werkzeug>=0.15 (from flask) Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 307kB 3.4MB/s Collecting itsdangerous>=0.24 (from flask) Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl Collecting click>=5.1 (from flask) Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 10.2MB/s Collecting Jinja2>=2.10.1 (from flask) Downloading https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl (125kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.5MB/s Collecting MarkupSafe>=0.23 (from Jinja2>=2.10.1->flask) Downloading https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl Installing collected packages: Werkzeug, itsdangerous, click, MarkupSafe, Jinja2, flask Successfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0 $ python3 app.py * Serving Flask app \"app\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) Step 2: Create and build the Docker Image \u00b6 Now, what if you don't have python installed locally? Don't worry! Because you don't need it. One of the advantages of using containers is that you can build python inside your containers, without having python installed on your host machine. Create a Dockerfile but running the following command. (copy-paste the entire code block) echo 'FROM python:3.8-alpine RUN pip install flask CMD [\"python\",\"app.py\"] COPY app.py /app.py' > Dockerfile A Dockerfile lists the instructions needed to build a docker image. Let's go through the above file line by line. FROM python:3.8-alpine This is the starting point for your Dockerfile. Every Dockerfile must start with a FROM line that is the starting image to build your layers on top of. In this case, we are selecting the python:3.8-alpine base layer (see Dockerfile for python3.8/alpine3.12 ) since it already has the version of python and pip that we need to run our application. The alpine version means that it uses the Alpine Linux distribution, which is significantly smaller than many alternative flavors of Linux, around 8 MB in size, while a minimal installation to disk might be around 130 MB. A smaller image means it will download (deploy) much faster, and it also has advantages for security because it has a smaller attack surface. Alpine Linux is a Linux distribution based on musl and BusyBox. Here we are using the \"3.8-alpine\" tag for the python image. Take a look at the available tags for the official python image on the Docker Hub . It is best practice to use a specific tag when inheriting a parent image so that changes to the parent dependency are controlled. If no tag is specified, the \"latest\" tag takes into effect, which is acts as a dynamic pointer that points to the latest version of an image. For security reasons, it is very important to understand the layers that you build your docker image on top of. For that reason, it is highly recommended to only use \"official\" images found in the docker hub , or non-community images found in the docker-store. These images are vetted to meet certain security requirements, and also have very good documentation for users to follow. You can find more information about this python base image , as well as all other images that you can use, on the docker hub . For a more complex application you may find the need to use a FROM image that is higher up the chain. For example, the parent Dockerfile for our python app starts with FROM alpine , then specifies a series of CMD and RUN commands for the image. If you needed more fine-grained control, you could start with FROM alpine (or a different distribution) and run those steps yourself. To start off though, I recommend using an official image that closely matches your needs. RUN pip install flask The RUN command executes commands needed to set up your image for your application, such as installing packages, editing files, or changing file permissions. In this case we are installing flask. The RUN commands are executed at build time, and are added to the layers of your image. CMD [\"python\",\"app.py\"] CMD is the command that is executed when you start a container. Here we are using CMD to run our python app. There can be only one CMD per Dockerfile. If you specify more thane one CMD , then the last CMD will take effect. The parent python:3.8-alpine also specifies a CMD ( CMD python3 ). You can find the Dockerfile for the official python:alpine image here . You can use the official python image directly to run python scripts without installing python on your host. But today, we are creating a custom image to include our source, so that we can build an image with our application and ship it around to other environments. COPY app.py /app.py This copies the app.py in the local directory (where you will run docker image build ) into a new layer of the image. This instruction is the last line in the Dockerfile. Layers that change frequently, such as copying source code into the image, should be placed near the bottom of the file to take full advantage of the Docker layer cache. This allows us to avoid rebuilding layers that could otherwise be cached. For instance, if there was a change in the FROM instruction, it would invalidate the cache for all subsequent layers of this image. We will demonstrate a this little later in this lab. It seems counter-intuitive to put this after the CMD [\"python\",\"app.py\"] line. Remember, the CMD line is executed only when the container is started, so we won't get a file not found error here. And there you have it: a very simple Dockerfile. A full list of commands you can put into a Dockerfile can be found here . Now that we defined our Dockerfile, let's use it to build our custom docker image. Build the docker image. Pass in -t to name your image python-hello-world . $ docker image build -t python-hello-world . Sending build context to Docker daemon 3 .072kB Step 1 /4 : FROM python:3.8-alpine 3 .8-alpine: Pulling from library/python df20fa9351a1: Pull complete 36b3adc4ff6f: Pull complete 3e7ef1bb9eba: Pull complete 78538f72d6a9: Pull complete 07bc731e0055: Pull complete Digest: sha256:cbc08bfc4b1b732076742f52852ede090e960ab7470d0a60ee4f964cfa7c710a Status: Downloaded newer image for python:3.8-alpine ---> 0f03316d4a27 Step 2 /4 : RUN pip install flask ---> Running in 1454bdd1ea98 Collecting flask Downloading Flask-1.1.2-py2.py3-none-any.whl ( 94 kB ) Collecting itsdangerous> = 0 .24 Downloading itsdangerous-1.1.0-py2.py3-none-any.whl ( 16 kB ) Collecting Werkzeug> = 0 .15 Downloading Werkzeug-1.0.1-py2.py3-none-any.whl ( 298 kB ) Collecting click> = 5 .1 Downloading click-7.1.2-py2.py3-none-any.whl ( 82 kB ) Collecting Jinja2> = 2 .10.1 Downloading Jinja2-2.11.2-py2.py3-none-any.whl ( 125 kB ) Collecting MarkupSafe> = 0 .23 Downloading MarkupSafe-1.1.1.tar.gz ( 19 kB ) Building wheels for collected packages: MarkupSafe Building wheel for MarkupSafe ( setup.py ) : started Building wheel for MarkupSafe ( setup.py ) : finished with status 'done' Created wheel for MarkupSafe: filename = MarkupSafe-1.1.1-py3-none-any.whl size = 12627 sha256 = 155e3314602dfac3c8ea245edc217c235afb4c818932574d6d61529ef0c14ea4 Stored in directory: /root/.cache/pip/wheels/0c/61/d6/4db4f4c28254856e82305fdb1f752ed7f8482e54c384d8cb0e Successfully built MarkupSafe Installing collected packages: itsdangerous, Werkzeug, click, MarkupSafe, Jinja2, flask Successfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0 Removing intermediate container 1454bdd1ea98 ---> 97d747fc7771 Step 3 /4 : CMD [ \"python\" , \"app.py\" ] ---> Running in e2bf74801c81 Removing intermediate container e2bf74801c81 ---> d5adbccf5116 Step 4 /4 : COPY app.py /app.py ---> 3c24958f29d3 Successfully built 3c24958f29d3 Successfully tagged python-hello-world:latest Verify that your image shows up in your image list via docker image ls . $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE python-hello-world latest 3c24958f29d3 52 seconds ago 53 .4MB python 3 .8-alpine 0f03316d4a27 2 weeks ago 42 .7MB Note that your base image python:3.8-alpine is also in your list. You can run a history command to show the history of an image and its layers, docker history python-hello-world docker history python:3.8-alpine Step 3: Run the Docker image \u00b6 Now that you have built the image, you can run it to see that it works. Run the Docker image $ docker run -p 5001 :5000 -d python-hello-world 0b2ba61df37fb4038d9ae5d145740c63c2c211ae2729fc27dc01b82b5aaafa26 The -p flag maps a port running inside the container to your host. In this case, we are mapping the python app running on port 5000 inside the container, to port 5001 on your host. Note that if port 5001 is already in use by another application on your host, you may have to replace 5001 with another value, such as 5002. Navigate to localhost:5001 in a browser to see the results. In a terminal run curl localhost:5001 , which returns hello world! . If you are using katacoda, click on the link in the left-hand pane that says: View port at https://....environments.katacoda.com then type in 5001 and click Display Port . In play-with-docker, click the link 5001 that should appear near the top of your session. You should see \"hello world!\" on your browser. Check the log output of the container. If you want to see logs from your application you can use the docker container logs command. By default, docker container logs prints out what is sent to standard out by your application. Use docker container ls to find the id for your running container. sh $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7b04d5320cb4 python-hello-world \"python app.py\" About a minute ago Up About a minute 0.0.0.0:5001->5000/tcp elastic_ganguly $ docker container logs [container id] * Serving Flask app \"app\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) 172.17.0.1 - - [23/Sep/2020 22:00:33] \"GET / HTTP/1.1\" 200 - The Dockerfile is how you create reproducible builds for your application. A common workflow is to have your CI/CD automation run docker image build as part of its build process. Once images are built, they will be sent to a central registry, where it can be accessed by all environments (such as a test environment) that need to run instances of that application. In the next step, we will push our custom image to the public docker registry: the docker hub, where it can be consumed by other developers and operators. Step 4: Push to a central registry \u00b6 Navigate to Docker Hub and create an account if you haven't already. Alternatively, you can also use https://quay.io for instance. For this lab we will be using the docker hub as our central registry. Docker hub is a free service to store publicly available images, or you can pay to store private images. Go to the Docker Hub website and create a free account. Most organizations that use docker heavily will set up their own registry internally. To simplify things, we will be using the Docker Hub, but the following concepts apply to any registry. Login You can log into the image registry account by typing docker login on your terminal, or if using podman, type podman login . $ export DOCKERHUB_USERNAME = <dockerhub-username> $ docker login docker.io -u $DOCKERHUB_USERNAME password: WARNING! Your password will be stored unencrypted in /home/theia/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Tag your image with your username The Docker Hub naming convention is to tag your image with [dockerhub username]/[image name]. To do this, we are going to tag our previously created image python-hello-world to fit that format. docker tag python-hello-world $DOCKERHUB_USERNAME /python-hello-world Push your image to the registry Once we have a properly tagged image, we can use the docker push command to push our image to the Docker Hub registry. $ docker push $DOCKERHUB_USERNAME /python-hello-world The push refers to a repository [ docker.io/jzaccone/python-hello-world ] 2bce026769ac: Pushed 64d445ecbe93: Pushed 18b27eac38a1: Mounted from library/python 3f6f25cd8b1e: Mounted from library/python b7af9d602a0f: Mounted from library/python ed06208397d5: Mounted from library/python 5accac14015f: Mounted from library/python latest: digest: sha256:508238f264616bf7bf962019d1a3826f8487ed6a48b80bf41fd3996c7175fd0f size: 1786 Check out your image on docker hub in your browser Navigate to Docker Hub and go to your profile to see your newly uploaded image at https://hub.docker.com/repository/docker/<dockerhub-username>/python-hello-world . Now that your image is on Docker Hub, other developers and operations can use the docker pull command to deploy your image to other environments. Note: Docker images contain all the dependencies that it needs to run an application within the image. This is useful because we no longer have deal with environment drift (version differences) when we rely on dependencies that are install on every environment we deploy to. We also don't have to go through additional steps to provision these environments. Just one step: install docker, and you are good to go. Step 5: Deploying a Change \u00b6 The \"hello world!\" application is overrated, let's update the app so that it says \"Hello Beautiful World!\" instead. Update app.py Replace the string \"Hello World\" with \"Hello Beautiful World!\" in app.py . You can update the file with the following command. (copy-paste the entire code block) echo 'from flask import Flask app = Flask(__name__) @app.route(\"/\") def hello(): return \"hello beautiful world!\" if __name__ == \"__main__\": app.run(host=\"0.0.0.0\")' > app.py Rebuild and push your image Now that your app is updated, you need repeat the steps above to rebuild your app and push it to the Docker Hub registry. First rebuild, this time use your Docker Hub username in the build command: $ docker image build -t $DOCKERHUB_USERNAME /python-hello-world . Sending build context to Docker daemon 3 .072kB Step 1 /4 : FROM python:3.6.1-alpine ---> c86415c03c37 Step 2 /4 : RUN pip install flask ---> Using cache ---> ce41f2517c16 Step 3 /4 : CMD python app.py ---> Using cache ---> 0ab91286958b Step 4 /4 : COPY app.py /app.py ---> 3e08b2eeace1 Removing intermediate container 23a955e881fc Successfully built 3e08b2eeace1 Successfully tagged <dockerhub-username>/python-hello-world:latest Notice the \"Using cache\" for steps 1-3. These layers of the Docker Image have already been built and docker image build will use these layers from the cache instead of rebuilding them. $ docker push $DOCKERHUB_USERNAME /python-hello-world The push refers to a repository [ docker.io/<dockerhub-username>/python-hello-world ] 94525867566e: Pushed 64d445ecbe93: Layer already exists 18b27eac38a1: Layer already exists 3f6f25cd8b1e: Layer already exists b7af9d602a0f: Layer already exists ed06208397d5: Layer already exists 5accac14015f: Layer already exists latest: digest: sha256:91874e88c14f217b4cab1dd5510da307bf7d9364bd39860c9cc8688573ab1a3a size: 1786 There is a caching mechanism in place for pushing layers too. Docker Hub already has all but one of the layers from an earlier push, so it only pushes the one layer that has changed. When you change a layer, every layer built on top of that will have to be rebuilt. Each line in a Dockerfile builds a new layer that is built on the layer created from the lines before it. This is why the order of the lines in our Dockerfile is important. We optimized our Dockerfile so that the layer that is most likely to change ( COPY app.py /app.py ) is the last line of the Dockerfile. Generally for an application, your code changes at the most frequent rate. This optimization is particularly important for CI/CD processes, where you want your automation to run as fast as possible. Step 6: Understanding Image Layers \u00b6 One of the major design properties of Docker is its use of the union file system. Consider the Dockerfile that we created before: FROM python:3.8-alpine RUN pip install flask CMD [ \"python\" , \"app.py\" ] COPY app.py /app.py Each of these lines is a layer. Each layer contains only the delta, diff or changes from the layers before it. To put these layers together into a single running container, Docker makes use of the union file system to overlay layers transparently into a single view. Each layer of the image is read-only , except for the very top layer which is created for the running container. The read/write container layer implements \"copy-on-write\" which means that files that are stored in lower image layers are pulled up to the read/write container layer only when edits are being made to those files. Those changes are then stored in the running container layer. The \"copy-on-write\" function is very fast, and in almost all cases, does not have a noticeable effect on performance. You can inspect which files have been pulled up to the container level with the docker diff command. More information about how to use docker diff can be found here . Since image layers are read-only , they can be shared by images and by running containers. For instance, creating a new python app with its own Dockerfile with similar base layers, would share all the layers that it had in common with the first python app. FROM python:3.8-alpine RUN pip install flask CMD [ \"python\" , \"app2.py\" ] COPY app2.py /app2.py You can also experience the sharing of layers when you start multiple containers from the same image. Since the containers use the same read-only layers, you can imagine that starting up containers is very fast and has a very low footprint on the host. You may notice that there are duplicate lines in this Dockerfile and the Dockerfile you created earlier in this lab. Although this is a very trivial example, you can pull common lines of both Dockerfiles into a \"base\" Dockerfile, that you can then point to with each of your child Dockerfiles using the FROM command. Image layering enables the docker caching mechanism for builds and pushes. For example, the output for your last docker push shows that some of the layers of your image already exists on the Docker Hub. $ docker push $DOCKERHUB_USERNAME /python-hello-world The push refers to a repository [ docker.io/<dockerhub-username>/python-hello-world ] 94525867566e: Pushed 64d445ecbe93: Layer already exists 18b27eac38a1: Layer already exists 3f6f25cd8b1e: Layer already exists b7af9d602a0f: Layer already exists ed06208397d5: Layer already exists 5accac14015f: Layer already exists latest: digest: sha256:91874e88c14f217b4cab1dd5510da307bf7d9364bd39860c9cc8688573ab1a3a size: 1786 To look more closely at layers, you can use the docker image history command of the python image we created. $ docker image history python-hello-world IMAGE CREATED CREATED BY SIZE COMMENT 3c24958f29d3 17 minutes ago /bin/sh -c #(nop) COPY file:5fef1b9a6220c0e3\u2026 159B d5adbccf5116 17 minutes ago /bin/sh -c #(nop) CMD [\"python\" \"app.py\"] 0B 97d747fc7771 17 minutes ago /bin/sh -c pip install flask 10.7MB 0f03316d4a27 2 weeks ago /bin/sh -c #(nop) CMD [\"python3\"] 0B <missing> 2 weeks ago /bin/sh -c set -ex; wget -O get-pip.py \"$P\u2026 7.24MB <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_GET_PIP_SHA256\u2026 0B <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_GET_PIP_URL=ht\u2026 0B <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_PIP_VERSION=20\u2026 0B <missing> 7 weeks ago /bin/sh -c cd /usr/local/bin && ln -s idle3\u2026 32B <missing> 7 weeks ago /bin/sh -c set -ex && apk add --no-cache --\u2026 29.3MB <missing> 2 months ago /bin/sh -c #(nop) ENV PYTHON_VERSION=3.8.5 0B <missing> 3 months ago /bin/sh -c #(nop) ENV GPG_KEY=E3FF2839C048B\u2026 0B <missing> 3 months ago /bin/sh -c apk add --no-cache ca-certificates 512kB <missing> 3 months ago /bin/sh -c #(nop) ENV LANG=C.UTF-8 0B <missing> 3 months ago /bin/sh -c #(nop) ENV PATH=/usr/local/bin:/\u2026 0B <missing> 3 months ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 3 months ago /bin/sh -c #(nop) ADD file:c92c248239f8c7b9b\u2026 5.57MB Each line represents a layer of the image. You'll notice that the top lines match to your Dockerfile that you created, and the lines below are pulled from the parent python image. Don't worry about the \"\\<missing>\" tags. These are still normal layers; they have just not been given an ID by the docker system. Step 7: Clean up \u00b6 Completing this lab results in a bunch of running containers on your host. Let's clean these up. Run docker container stop [container id] for each container that is running First get a list of the containers running using docker container ls . $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0b2ba61df37f python-hello-world \"python app.py\" 7 minutes ago Up 7 minutes 0 .0.0.0:5001->5000/tcp practical_kirch Then run docker container stop [container id] for each container in the list. $ docker container stop 0b2 0b2 Remove the stopped containers docker system prune is a really handy command to clean up your system. It will remove any stopped containers, unused volumes and networks, and dangling images. $ docker system prune WARNING! This will remove: - all stopped containers - all volumes not used by at least one container - all networks not used by at least one container - all dangling images Are you sure you want to continue ? [ y/N ] y Deleted Containers: 0b2ba61df37fb4038d9ae5d145740c63c2c211ae2729fc27dc01b82b5aaafa26 Total reclaimed space: 300 .3kB Summary \u00b6 In this lab, you started adding value by creating your own custom docker containers. Key Takeaways: The Dockerfile is how you create reproducible builds for your application and how you integrate your application with Docker into the CI/CD pipeline Docker images can be made available to all of your environments through a central registry. The Docker Hub is one example of a registry, but you can deploy your own registry on servers you control. Docker images contain all the dependencies that it needs to run an application within the image. This is useful because we no longer have deal with environment drift (version differences) when we rely on dependencies that are install on every environment we deploy to. Docker makes use of the union file system and \"copy on write\" to reuse layers of images. This lowers the footprint of storing images and significantly increases the performance of starting containers. Image layers are cached by the Docker build and push system. No need to rebuild or repush image layers that are already present on the desired system. Each line in a Dockerfile creates a new layer, and because of the layer cache, the lines that change more frequently (e.g. adding source code to an image) should be listed near the bottom of the file.","title":"Lab 2. Adding Value with Custom Docker Images"},{"location":"lab-2/#lab-2-adding-value-with-custom-docker-images","text":"","title":"Lab 2 - Adding Value with Custom Docker Images"},{"location":"lab-2/#overview","text":"In this lab, we build on our knowledge from lab 1 where we used Docker commands to run containers. We will create a custom Docker Image built from a Dockerfile. Once we build the image, we will push it to a central registry where it can be pulled to be deployed on other environments. Also, we will briefly describe image layers, and how Docker incorporates \"copy-on-write\" and the union file system to efficiently store images and run containers. We will be using a few Docker commands in this lab. For full documentation on available commands check out the official documentation .","title":"Overview"},{"location":"lab-2/#prerequisites","text":"Completed Lab 0: You must have access to a docker client, either on localhost, use a terminal from Theia - Cloud IDE at https://labs.cognitiveclass.ai/tools/theiadocker or be using Play with Docker for example.","title":"Prerequisites"},{"location":"lab-2/#step-1-create-a-python-app-without-using-docker","text":"Run the following command to create a file named app.py with a simple python program. (copy-paste the entire code block) echo 'from flask import Flask app = Flask(__name__) @app.route(\"/\") def hello(): return \"hello world!\" if __name__ == \"__main__\": app.run(host=\"0.0.0.0\")' > app.py This is a simple python app that uses flask to expose a http web server on port 5000 (5000 is the default port for flask). Don't worry if you are not too familiar with python or flask, these concepts can be applied to an application written in any language. Optional: If you have python and pip installed, you can run this app locally. If not, move on to the next step. $ python3 --version Python 3.6.9 $ pip3 --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6) $ pip3 install flask Collecting flask Downloading https://files.pythonhosted.org/packages/f2/28/2a03252dfb9ebf377f40fba6a7841b47083260bf8bd8e737b0c6952df83f/Flask-1.1.2-py2.py3-none-any.whl (94kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 6.2MB/s Collecting Werkzeug>=0.15 (from flask) Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 307kB 3.4MB/s Collecting itsdangerous>=0.24 (from flask) Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl Collecting click>=5.1 (from flask) Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 10.2MB/s Collecting Jinja2>=2.10.1 (from flask) Downloading https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl (125kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.5MB/s Collecting MarkupSafe>=0.23 (from Jinja2>=2.10.1->flask) Downloading https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl Installing collected packages: Werkzeug, itsdangerous, click, MarkupSafe, Jinja2, flask Successfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0 $ python3 app.py * Serving Flask app \"app\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)","title":"Step 1: Create a python app (without using Docker)"},{"location":"lab-2/#step-2-create-and-build-the-docker-image","text":"Now, what if you don't have python installed locally? Don't worry! Because you don't need it. One of the advantages of using containers is that you can build python inside your containers, without having python installed on your host machine. Create a Dockerfile but running the following command. (copy-paste the entire code block) echo 'FROM python:3.8-alpine RUN pip install flask CMD [\"python\",\"app.py\"] COPY app.py /app.py' > Dockerfile A Dockerfile lists the instructions needed to build a docker image. Let's go through the above file line by line. FROM python:3.8-alpine This is the starting point for your Dockerfile. Every Dockerfile must start with a FROM line that is the starting image to build your layers on top of. In this case, we are selecting the python:3.8-alpine base layer (see Dockerfile for python3.8/alpine3.12 ) since it already has the version of python and pip that we need to run our application. The alpine version means that it uses the Alpine Linux distribution, which is significantly smaller than many alternative flavors of Linux, around 8 MB in size, while a minimal installation to disk might be around 130 MB. A smaller image means it will download (deploy) much faster, and it also has advantages for security because it has a smaller attack surface. Alpine Linux is a Linux distribution based on musl and BusyBox. Here we are using the \"3.8-alpine\" tag for the python image. Take a look at the available tags for the official python image on the Docker Hub . It is best practice to use a specific tag when inheriting a parent image so that changes to the parent dependency are controlled. If no tag is specified, the \"latest\" tag takes into effect, which is acts as a dynamic pointer that points to the latest version of an image. For security reasons, it is very important to understand the layers that you build your docker image on top of. For that reason, it is highly recommended to only use \"official\" images found in the docker hub , or non-community images found in the docker-store. These images are vetted to meet certain security requirements, and also have very good documentation for users to follow. You can find more information about this python base image , as well as all other images that you can use, on the docker hub . For a more complex application you may find the need to use a FROM image that is higher up the chain. For example, the parent Dockerfile for our python app starts with FROM alpine , then specifies a series of CMD and RUN commands for the image. If you needed more fine-grained control, you could start with FROM alpine (or a different distribution) and run those steps yourself. To start off though, I recommend using an official image that closely matches your needs. RUN pip install flask The RUN command executes commands needed to set up your image for your application, such as installing packages, editing files, or changing file permissions. In this case we are installing flask. The RUN commands are executed at build time, and are added to the layers of your image. CMD [\"python\",\"app.py\"] CMD is the command that is executed when you start a container. Here we are using CMD to run our python app. There can be only one CMD per Dockerfile. If you specify more thane one CMD , then the last CMD will take effect. The parent python:3.8-alpine also specifies a CMD ( CMD python3 ). You can find the Dockerfile for the official python:alpine image here . You can use the official python image directly to run python scripts without installing python on your host. But today, we are creating a custom image to include our source, so that we can build an image with our application and ship it around to other environments. COPY app.py /app.py This copies the app.py in the local directory (where you will run docker image build ) into a new layer of the image. This instruction is the last line in the Dockerfile. Layers that change frequently, such as copying source code into the image, should be placed near the bottom of the file to take full advantage of the Docker layer cache. This allows us to avoid rebuilding layers that could otherwise be cached. For instance, if there was a change in the FROM instruction, it would invalidate the cache for all subsequent layers of this image. We will demonstrate a this little later in this lab. It seems counter-intuitive to put this after the CMD [\"python\",\"app.py\"] line. Remember, the CMD line is executed only when the container is started, so we won't get a file not found error here. And there you have it: a very simple Dockerfile. A full list of commands you can put into a Dockerfile can be found here . Now that we defined our Dockerfile, let's use it to build our custom docker image. Build the docker image. Pass in -t to name your image python-hello-world . $ docker image build -t python-hello-world . Sending build context to Docker daemon 3 .072kB Step 1 /4 : FROM python:3.8-alpine 3 .8-alpine: Pulling from library/python df20fa9351a1: Pull complete 36b3adc4ff6f: Pull complete 3e7ef1bb9eba: Pull complete 78538f72d6a9: Pull complete 07bc731e0055: Pull complete Digest: sha256:cbc08bfc4b1b732076742f52852ede090e960ab7470d0a60ee4f964cfa7c710a Status: Downloaded newer image for python:3.8-alpine ---> 0f03316d4a27 Step 2 /4 : RUN pip install flask ---> Running in 1454bdd1ea98 Collecting flask Downloading Flask-1.1.2-py2.py3-none-any.whl ( 94 kB ) Collecting itsdangerous> = 0 .24 Downloading itsdangerous-1.1.0-py2.py3-none-any.whl ( 16 kB ) Collecting Werkzeug> = 0 .15 Downloading Werkzeug-1.0.1-py2.py3-none-any.whl ( 298 kB ) Collecting click> = 5 .1 Downloading click-7.1.2-py2.py3-none-any.whl ( 82 kB ) Collecting Jinja2> = 2 .10.1 Downloading Jinja2-2.11.2-py2.py3-none-any.whl ( 125 kB ) Collecting MarkupSafe> = 0 .23 Downloading MarkupSafe-1.1.1.tar.gz ( 19 kB ) Building wheels for collected packages: MarkupSafe Building wheel for MarkupSafe ( setup.py ) : started Building wheel for MarkupSafe ( setup.py ) : finished with status 'done' Created wheel for MarkupSafe: filename = MarkupSafe-1.1.1-py3-none-any.whl size = 12627 sha256 = 155e3314602dfac3c8ea245edc217c235afb4c818932574d6d61529ef0c14ea4 Stored in directory: /root/.cache/pip/wheels/0c/61/d6/4db4f4c28254856e82305fdb1f752ed7f8482e54c384d8cb0e Successfully built MarkupSafe Installing collected packages: itsdangerous, Werkzeug, click, MarkupSafe, Jinja2, flask Successfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0 Removing intermediate container 1454bdd1ea98 ---> 97d747fc7771 Step 3 /4 : CMD [ \"python\" , \"app.py\" ] ---> Running in e2bf74801c81 Removing intermediate container e2bf74801c81 ---> d5adbccf5116 Step 4 /4 : COPY app.py /app.py ---> 3c24958f29d3 Successfully built 3c24958f29d3 Successfully tagged python-hello-world:latest Verify that your image shows up in your image list via docker image ls . $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE python-hello-world latest 3c24958f29d3 52 seconds ago 53 .4MB python 3 .8-alpine 0f03316d4a27 2 weeks ago 42 .7MB Note that your base image python:3.8-alpine is also in your list. You can run a history command to show the history of an image and its layers, docker history python-hello-world docker history python:3.8-alpine","title":"Step 2: Create and build the Docker Image"},{"location":"lab-2/#step-3-run-the-docker-image","text":"Now that you have built the image, you can run it to see that it works. Run the Docker image $ docker run -p 5001 :5000 -d python-hello-world 0b2ba61df37fb4038d9ae5d145740c63c2c211ae2729fc27dc01b82b5aaafa26 The -p flag maps a port running inside the container to your host. In this case, we are mapping the python app running on port 5000 inside the container, to port 5001 on your host. Note that if port 5001 is already in use by another application on your host, you may have to replace 5001 with another value, such as 5002. Navigate to localhost:5001 in a browser to see the results. In a terminal run curl localhost:5001 , which returns hello world! . If you are using katacoda, click on the link in the left-hand pane that says: View port at https://....environments.katacoda.com then type in 5001 and click Display Port . In play-with-docker, click the link 5001 that should appear near the top of your session. You should see \"hello world!\" on your browser. Check the log output of the container. If you want to see logs from your application you can use the docker container logs command. By default, docker container logs prints out what is sent to standard out by your application. Use docker container ls to find the id for your running container. sh $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7b04d5320cb4 python-hello-world \"python app.py\" About a minute ago Up About a minute 0.0.0.0:5001->5000/tcp elastic_ganguly $ docker container logs [container id] * Serving Flask app \"app\" (lazy loading) * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) 172.17.0.1 - - [23/Sep/2020 22:00:33] \"GET / HTTP/1.1\" 200 - The Dockerfile is how you create reproducible builds for your application. A common workflow is to have your CI/CD automation run docker image build as part of its build process. Once images are built, they will be sent to a central registry, where it can be accessed by all environments (such as a test environment) that need to run instances of that application. In the next step, we will push our custom image to the public docker registry: the docker hub, where it can be consumed by other developers and operators.","title":"Step 3: Run the Docker image"},{"location":"lab-2/#step-4-push-to-a-central-registry","text":"Navigate to Docker Hub and create an account if you haven't already. Alternatively, you can also use https://quay.io for instance. For this lab we will be using the docker hub as our central registry. Docker hub is a free service to store publicly available images, or you can pay to store private images. Go to the Docker Hub website and create a free account. Most organizations that use docker heavily will set up their own registry internally. To simplify things, we will be using the Docker Hub, but the following concepts apply to any registry. Login You can log into the image registry account by typing docker login on your terminal, or if using podman, type podman login . $ export DOCKERHUB_USERNAME = <dockerhub-username> $ docker login docker.io -u $DOCKERHUB_USERNAME password: WARNING! Your password will be stored unencrypted in /home/theia/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Tag your image with your username The Docker Hub naming convention is to tag your image with [dockerhub username]/[image name]. To do this, we are going to tag our previously created image python-hello-world to fit that format. docker tag python-hello-world $DOCKERHUB_USERNAME /python-hello-world Push your image to the registry Once we have a properly tagged image, we can use the docker push command to push our image to the Docker Hub registry. $ docker push $DOCKERHUB_USERNAME /python-hello-world The push refers to a repository [ docker.io/jzaccone/python-hello-world ] 2bce026769ac: Pushed 64d445ecbe93: Pushed 18b27eac38a1: Mounted from library/python 3f6f25cd8b1e: Mounted from library/python b7af9d602a0f: Mounted from library/python ed06208397d5: Mounted from library/python 5accac14015f: Mounted from library/python latest: digest: sha256:508238f264616bf7bf962019d1a3826f8487ed6a48b80bf41fd3996c7175fd0f size: 1786 Check out your image on docker hub in your browser Navigate to Docker Hub and go to your profile to see your newly uploaded image at https://hub.docker.com/repository/docker/<dockerhub-username>/python-hello-world . Now that your image is on Docker Hub, other developers and operations can use the docker pull command to deploy your image to other environments. Note: Docker images contain all the dependencies that it needs to run an application within the image. This is useful because we no longer have deal with environment drift (version differences) when we rely on dependencies that are install on every environment we deploy to. We also don't have to go through additional steps to provision these environments. Just one step: install docker, and you are good to go.","title":"Step 4: Push to a central registry"},{"location":"lab-2/#step-5-deploying-a-change","text":"The \"hello world!\" application is overrated, let's update the app so that it says \"Hello Beautiful World!\" instead. Update app.py Replace the string \"Hello World\" with \"Hello Beautiful World!\" in app.py . You can update the file with the following command. (copy-paste the entire code block) echo 'from flask import Flask app = Flask(__name__) @app.route(\"/\") def hello(): return \"hello beautiful world!\" if __name__ == \"__main__\": app.run(host=\"0.0.0.0\")' > app.py Rebuild and push your image Now that your app is updated, you need repeat the steps above to rebuild your app and push it to the Docker Hub registry. First rebuild, this time use your Docker Hub username in the build command: $ docker image build -t $DOCKERHUB_USERNAME /python-hello-world . Sending build context to Docker daemon 3 .072kB Step 1 /4 : FROM python:3.6.1-alpine ---> c86415c03c37 Step 2 /4 : RUN pip install flask ---> Using cache ---> ce41f2517c16 Step 3 /4 : CMD python app.py ---> Using cache ---> 0ab91286958b Step 4 /4 : COPY app.py /app.py ---> 3e08b2eeace1 Removing intermediate container 23a955e881fc Successfully built 3e08b2eeace1 Successfully tagged <dockerhub-username>/python-hello-world:latest Notice the \"Using cache\" for steps 1-3. These layers of the Docker Image have already been built and docker image build will use these layers from the cache instead of rebuilding them. $ docker push $DOCKERHUB_USERNAME /python-hello-world The push refers to a repository [ docker.io/<dockerhub-username>/python-hello-world ] 94525867566e: Pushed 64d445ecbe93: Layer already exists 18b27eac38a1: Layer already exists 3f6f25cd8b1e: Layer already exists b7af9d602a0f: Layer already exists ed06208397d5: Layer already exists 5accac14015f: Layer already exists latest: digest: sha256:91874e88c14f217b4cab1dd5510da307bf7d9364bd39860c9cc8688573ab1a3a size: 1786 There is a caching mechanism in place for pushing layers too. Docker Hub already has all but one of the layers from an earlier push, so it only pushes the one layer that has changed. When you change a layer, every layer built on top of that will have to be rebuilt. Each line in a Dockerfile builds a new layer that is built on the layer created from the lines before it. This is why the order of the lines in our Dockerfile is important. We optimized our Dockerfile so that the layer that is most likely to change ( COPY app.py /app.py ) is the last line of the Dockerfile. Generally for an application, your code changes at the most frequent rate. This optimization is particularly important for CI/CD processes, where you want your automation to run as fast as possible.","title":"Step 5: Deploying a Change"},{"location":"lab-2/#step-6-understanding-image-layers","text":"One of the major design properties of Docker is its use of the union file system. Consider the Dockerfile that we created before: FROM python:3.8-alpine RUN pip install flask CMD [ \"python\" , \"app.py\" ] COPY app.py /app.py Each of these lines is a layer. Each layer contains only the delta, diff or changes from the layers before it. To put these layers together into a single running container, Docker makes use of the union file system to overlay layers transparently into a single view. Each layer of the image is read-only , except for the very top layer which is created for the running container. The read/write container layer implements \"copy-on-write\" which means that files that are stored in lower image layers are pulled up to the read/write container layer only when edits are being made to those files. Those changes are then stored in the running container layer. The \"copy-on-write\" function is very fast, and in almost all cases, does not have a noticeable effect on performance. You can inspect which files have been pulled up to the container level with the docker diff command. More information about how to use docker diff can be found here . Since image layers are read-only , they can be shared by images and by running containers. For instance, creating a new python app with its own Dockerfile with similar base layers, would share all the layers that it had in common with the first python app. FROM python:3.8-alpine RUN pip install flask CMD [ \"python\" , \"app2.py\" ] COPY app2.py /app2.py You can also experience the sharing of layers when you start multiple containers from the same image. Since the containers use the same read-only layers, you can imagine that starting up containers is very fast and has a very low footprint on the host. You may notice that there are duplicate lines in this Dockerfile and the Dockerfile you created earlier in this lab. Although this is a very trivial example, you can pull common lines of both Dockerfiles into a \"base\" Dockerfile, that you can then point to with each of your child Dockerfiles using the FROM command. Image layering enables the docker caching mechanism for builds and pushes. For example, the output for your last docker push shows that some of the layers of your image already exists on the Docker Hub. $ docker push $DOCKERHUB_USERNAME /python-hello-world The push refers to a repository [ docker.io/<dockerhub-username>/python-hello-world ] 94525867566e: Pushed 64d445ecbe93: Layer already exists 18b27eac38a1: Layer already exists 3f6f25cd8b1e: Layer already exists b7af9d602a0f: Layer already exists ed06208397d5: Layer already exists 5accac14015f: Layer already exists latest: digest: sha256:91874e88c14f217b4cab1dd5510da307bf7d9364bd39860c9cc8688573ab1a3a size: 1786 To look more closely at layers, you can use the docker image history command of the python image we created. $ docker image history python-hello-world IMAGE CREATED CREATED BY SIZE COMMENT 3c24958f29d3 17 minutes ago /bin/sh -c #(nop) COPY file:5fef1b9a6220c0e3\u2026 159B d5adbccf5116 17 minutes ago /bin/sh -c #(nop) CMD [\"python\" \"app.py\"] 0B 97d747fc7771 17 minutes ago /bin/sh -c pip install flask 10.7MB 0f03316d4a27 2 weeks ago /bin/sh -c #(nop) CMD [\"python3\"] 0B <missing> 2 weeks ago /bin/sh -c set -ex; wget -O get-pip.py \"$P\u2026 7.24MB <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_GET_PIP_SHA256\u2026 0B <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_GET_PIP_URL=ht\u2026 0B <missing> 2 weeks ago /bin/sh -c #(nop) ENV PYTHON_PIP_VERSION=20\u2026 0B <missing> 7 weeks ago /bin/sh -c cd /usr/local/bin && ln -s idle3\u2026 32B <missing> 7 weeks ago /bin/sh -c set -ex && apk add --no-cache --\u2026 29.3MB <missing> 2 months ago /bin/sh -c #(nop) ENV PYTHON_VERSION=3.8.5 0B <missing> 3 months ago /bin/sh -c #(nop) ENV GPG_KEY=E3FF2839C048B\u2026 0B <missing> 3 months ago /bin/sh -c apk add --no-cache ca-certificates 512kB <missing> 3 months ago /bin/sh -c #(nop) ENV LANG=C.UTF-8 0B <missing> 3 months ago /bin/sh -c #(nop) ENV PATH=/usr/local/bin:/\u2026 0B <missing> 3 months ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 3 months ago /bin/sh -c #(nop) ADD file:c92c248239f8c7b9b\u2026 5.57MB Each line represents a layer of the image. You'll notice that the top lines match to your Dockerfile that you created, and the lines below are pulled from the parent python image. Don't worry about the \"\\<missing>\" tags. These are still normal layers; they have just not been given an ID by the docker system.","title":"Step 6: Understanding Image Layers"},{"location":"lab-2/#step-7-clean-up","text":"Completing this lab results in a bunch of running containers on your host. Let's clean these up. Run docker container stop [container id] for each container that is running First get a list of the containers running using docker container ls . $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0b2ba61df37f python-hello-world \"python app.py\" 7 minutes ago Up 7 minutes 0 .0.0.0:5001->5000/tcp practical_kirch Then run docker container stop [container id] for each container in the list. $ docker container stop 0b2 0b2 Remove the stopped containers docker system prune is a really handy command to clean up your system. It will remove any stopped containers, unused volumes and networks, and dangling images. $ docker system prune WARNING! This will remove: - all stopped containers - all volumes not used by at least one container - all networks not used by at least one container - all dangling images Are you sure you want to continue ? [ y/N ] y Deleted Containers: 0b2ba61df37fb4038d9ae5d145740c63c2c211ae2729fc27dc01b82b5aaafa26 Total reclaimed space: 300 .3kB","title":"Step 7: Clean up"},{"location":"lab-2/#summary","text":"In this lab, you started adding value by creating your own custom docker containers. Key Takeaways: The Dockerfile is how you create reproducible builds for your application and how you integrate your application with Docker into the CI/CD pipeline Docker images can be made available to all of your environments through a central registry. The Docker Hub is one example of a registry, but you can deploy your own registry on servers you control. Docker images contain all the dependencies that it needs to run an application within the image. This is useful because we no longer have deal with environment drift (version differences) when we rely on dependencies that are install on every environment we deploy to. Docker makes use of the union file system and \"copy on write\" to reuse layers of images. This lowers the footprint of storing images and significantly increases the performance of starting containers. Image layers are cached by the Docker build and push system. No need to rebuild or repush image layers that are already present on the desired system. Each line in a Dockerfile creates a new layer, and because of the layer cache, the lines that change more frequently (e.g. adding source code to an image) should be listed near the bottom of the file.","title":"Summary"},{"location":"lab-3/","text":"Lab 3 - Managing Data in Containers \u00b6 Overview \u00b6 By default all files created inside a container are stored on a writable container layer. That means that: If the container no longer exists, the data is lost, The container's writable layer is tightly coupled to the host machine, and To manage the file system, you need a storage driver that provides a union file system, using the Linux kernel. This extra abstraction reduces performance compared to data volumes which write directly to the filesystem. Docker provides two options to store files in the host machine: volumes and bind mounts . If you're running Docker on Linux, you can also use a tmpfs mount , and with Docker on Windows you can also use a named pipe . Volumes are stored in the host filesystem that is managed by Docker. Bind mounts are stored anywhere on the host system. tmpfs mounts are stored in the host memory only. Originally, the --mount flag was used for Docker Swarm services and the --volume flag was used for standalone containers. From Docker 17.06 and higher, you can also use --mount for standalone containers and it is in general more explicit and verbose than --volume . Volumes \u00b6 A data volume or volume is a directory that bypasses the Union File System of Docker. There are three types of volumes: anonymous volume, named volume, and host volume. Anonymous Volume \u00b6 Let's create an instance of a popular open source NoSQL database called CouchDB and use an anonymous volume to store the data files for the database. To run an instance of CouchDB, use the CouchDB image from Docker Hub at https://hub.docker.com/_/couchdb . The docs say that the default for CouchDB is to write the database files to disk on the host system using its own internal volume management . Run the following command, docker run -d -p 5984:5984 --name my-couchdb -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 CouchDB will create an anonymous volume and generated a hashed name. Check the volumes on your host system, $ docker volume ls DRIVER VOLUME NAME local f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50 Set an environment variable VOLUME with the value of the generated name, export VOLUME=f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50 And inspect the volume that was created, use the hash name that was generated for the volume, $ docker volume inspect $VOLUME [ { \"CreatedAt\": \"2020-09-24T14:10:07Z\", \"Driver\": \"local\", \"Labels\": null, \"Mountpoint\": \"/var/lib/docker/volumes/f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50/_data\", \"Name\": \"f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50\", \"Options\": null, \"Scope\": \"local\" } ] You see that Docker has created and manages a volume in the Docker host filesystem under /var/lib/docker/volumes/$VOLUME_NAME/_data . Note that this is not a path on the host machine, but a part of the Docker managed filesystem. Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' Stop the container and start the container again, docker stop my-couchdb docker start my-couchdb Retrieve the document in the database to test that the data was persisted, $ curl -X GET -u admin:passw0rd1 http://127.0.0.1:5984/mydb/_all_docs {\"total_rows\":1,\"offset\":0,\"rows\":[ {\"id\":\"1\",\"key\":\"1\",\"value\":{\"rev\":\"1-c09289617e06b96bc747fb1201fea7f1\"}} ]} $ curl -X GET -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 {\"_id\":\"1\",\"_rev\":\"1-c09289617e06b96bc747fb1201fea7f1\",\"msg\":\"hello world\"} Sharing Volumes \u00b6 You can share an anonymous volume with another container by using the --volumes-from option. Create a busybox container with an anonymous volume mounted to a directory /data in the container, and using shell commands, write a message to a log file. $ docker run -it --name busybox1 -v /data busybox sh / # echo \"hello from busybox1\" > /data/hi.log / # ls /data hi.log / # exit Make sure the container busybox1 is stopped but not removed. $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 437fb4a271c1 busybox \"sh\" 18 seconds ago Exited (0) 4 seconds ago busybox1 Then create a second busybox container named busybox2 using the --volumes-from option to share the volume created by busybox1 , $ docker run --rm -it --name busybox2 --volumes-from busybox1 busybox sh / # ls -al /data / # cat /data/hi.log hello from busybox1 / # exit Docker created the anynomous volume that you were able to share using the --volumes-from option, and created a new anonymous volume. $ docker volume ls DRIVER VOLUME NAME local 83a3275e889506f3e8ff12cd50f7d5b501c1ace95672334597f9a071df439493 local f4e6b9f9568eeb165a56b2946847035414f5f9c2cad9ff79f18e800277ae1ebd Cleanup the existing volumes and container. docker stop my-couchdb docker rm my-couchdb docker rm busybox1 docker volume rm $(docker volume ls -q) docker system prune -a clear Named Volume \u00b6 A named volume and anonymous volume are similar in that Docker manages where they are located. However, a named volume can be referenced by name when mounting it to a container directory. This is helpful if you want to share a volume across multiple containers. First, create a named volume , docker volume create my-couchdb-data-volume Verify the volume was created, $ docker volume ls DRIVER VOLUME NAME local my-couchdb-data-volume Now create the CouchDB container using the named volume , docker run -d -p 5984:5984 --name my-couchdb -v my-couchdb-data-volume:/opt/couchdb/data -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 Wait until the CouchDB container is running and the instance is available. Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' It now is easy to share the volume with another container. For instance, read the content of the volume using the busybox image, and share the my-couchdb-data-volume volume by mounting the volume to a directory in the busybox container. $ docker run --rm -it --name busybox -v my-couchdb-data-volume:/myvolume busybox sh / # ls -al /myvolume/ total 40 drwxr-xr-x 4 5984 5984 4096 Sep 24 17:11 . drwxr-xr-x 1 root root 4096 Sep 24 17:14 .. drwxr-xr-x 2 5984 5984 4096 Sep 24 17:11 .delete -rw-r--r-- 1 5984 5984 8388 Sep 24 17:11 _dbs.couch -rw-r--r-- 1 5984 5984 8385 Sep 24 17:11 _nodes.couch drwxr-xr-x 4 5984 5984 4096 Sep 24 17:11 shards / # exit You can check the Docker managed filesystem for volumes by running a busybox container with privileged permission and set the process id to host to inspect the host system, and browse to the Docker managed directories. docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/volumes total 28 -rw------- 1 root root 32768 Nov 10 15:54 metadata.db drwxr-xr-x 3 root root 4096 Nov 10 15:54 my-couchdb-data-volume / # exit Cleanup, docker stop my-couchdb docker rm my-couchdb docker volume rm my-couchdb-data-volume docker system prune -a docker volume prune clear Host Volume \u00b6 When you want to access the volume directory easily from the host machine directly instead of using the Docker managed directories, you can create a host volume . Let's use a directory in the current working directory (indicated with the command pwd ) called data , or choose your own data directory on the host machine, e.g. /home/couchdb/data . We let docker create the $(pwd)/data directory if it does not exist yet. We mount the host volume inside the CouchDB container to the container directory /opt/couchdb/data , which is the default data directory for CouchDB. Run the following command, docker run -d -p 5984:5984 --name my-couchdb -v $(pwd)/data:/opt/couchdb/data -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 Verify that a directory data was created, $ ls -al total 16 drwxrwsrwx 3 root users 4096 Sep 24 16:27 . drwxrwxr-x 1 root root 4096 Jul 16 20:04 .. drwxr-sr-x 3 5984 5984 4096 Sep 24 16:27 data and that CouchDB has created data files here, $ ls -al data total 32 drwxr-sr-x 3 5984 5984 4096 Sep 24 16:27 . drwxrwsrwx 3 root users 4096 Sep 24 16:27 .. -rw-r--r-- 1 5984 5984 4257 Sep 24 16:27 _dbs.couch drwxr-sr-x 2 5984 5984 4096 Sep 24 16:27 .delete -rw-r--r-- 1 5984 5984 8385 Sep 24 16:27 _nodes.couch Also check that now, no managed volume was created by docker, because we are now using a host volume . docker volume ls and docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/volumes total 24 -rw------- 1 root root 32768 Nov 10 16:00 metadata.db / # exit Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' Note that CouchDB created a folder shards , $ ls -al data total 40 drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 . drwxrwsrwx 3 root users 4096 Sep 24 16:49 .. -rw-r--r-- 1 5984 5984 8388 Sep 24 16:49 _dbs.couch drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 .delete -rw-r--r-- 1 5984 5984 8385 Sep 24 16:49 _nodes.couch drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 shards List the content of the shards directory, $ ls -al data/shards total 16 drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 . drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 .. drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 00000000-7fffffff drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 80000000-ffffffff and the first shard, $ ls -al data/shards/00000000-7fffffff/ total 20 drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 . drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 .. -rw-r--r-- 1 5984 5984 8346 Sep 24 16:49 mydb.1600966173.couch A shard is a horizontal partition of data in a database. Partitioning data into shards and distributing copies of each shard to different nodes in a cluster gives the data greater durability against node loss. CouchDB automatically shards databases and distributes the subsets of documents among nodes. Cleanup, docker stop my-couchdb docker rm my-couchdb sudo rm -rf $(pwd)/data docker system prune -a Bind Mounts \u00b6 The mount syntax is recommended by Docker over the volume syntax. Bind mounts have limited functionality compared to volumes. A file or directory is referenced by its full path on the host machine when mounted into a container. Bind mounts rely on the host machine\u2019s filesystem having a specific directory structure available and you cannot use the Docker CLI to manage bind mounts. Note that bind mounts can change the host filesystem via processes running in a container. Instead of using the -v syntax with three fields separated by colon separator (:), the mount syntax is more verbose and uses multiple key-value pairs: type: bind, volume or tmpfs, source: path to the file or directory on host machine, destination: path in container, readonly, bind-propagation: rprivate, private, rshared, shared, rslave, slave, consistency: consistent, delegated, cached, mount. mkdir data docker run -it --name busybox --mount type=bind,source=\"$(pwd)\"/data,target=/data busybox sh / # echo \"hello busybox\" > /data/hi.txt / # exit cat data/hi.txt [Optional] OverlayFS \u00b6 OverlayFS is a union mount filesystem implementation for Linux. To understand what a Docker volume is, it helps to understand how layers and the filesystem work in Docker. To start a container, Docker takes the read-only image and creates a new read-write layer on top. To view the layers as one, Docker uses a Union File System or OverlayFS (Overlay File System), specifically the overlay2 storage driver. To see Docker host managed files, you need access to the Docker process file system. Using the --privileged and --pid=host flags you can access the host's process ID namespace from inside a container like busybox . You can then browse to Docker's /var/lib/docker/overlay2 directory to see the downloaded layers that are managed by Docker. To view the current list of layers in Docker, $ docker run -it --privileged --pid = host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/overlay2 total 16 drwx------ 3 root root 4096 Sep 25 19:44 0e55ecaa4d17c353191e68022d9a17fde64fb5e9217b07b5c56eb4c74dad5b32 drwx------ 5 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d drwx------ 4 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d-init drwx------ 2 root root 4096 Sep 25 19:44 l / # exit Pull down the ubuntu image and check again, $ docker pull ubuntu Using default tag: latest latest: Pulling from library/ubuntu e6ca3592b144: Pull complete 534a5505201d: Pull complete 990916bd23bb: Pull complete Digest: sha256:cbcf86d7781dbb3a6aa2bcea25403f6b0b443e20b9959165cf52d2cc9608e4b9 Status: Downloaded newer image for ubuntu:latest $ docker run -it --privileged --pid = host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/overlay2/ total 36 drwx------ 3 root root 4096 Sep 25 19:44 0e55ecaa4d17c353191e68022d9a17fde64fb5e9217b07b5c56eb4c74dad5b32 drwx------ 4 root root 4096 Sep 25 19:45 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d drwx------ 4 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d-init drwx------ 4 root root 4096 Sep 25 19:46 a611792b4cac502995fa88a888261dfba0b5d852e72f9db9e075050991423779 drwx------ 3 root root 4096 Sep 25 19:46 d181f1a41fc35a45c16e8bfcb8eee6f768f3b98f82210a43ea65f284a45fcd65 drwx------ 4 root root 4096 Sep 25 19:46 dac2f37f6280a076836d39b87b0ae5ebf5c0d386b6d8b991b103aadbcebaa7c6 drwx------ 5 root root 4096 Sep 25 19:47 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709 drwx------ 4 root root 4096 Sep 25 19:47 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709-init drwx------ 2 root root 4096 Sep 25 19:47 l / # exit You see that pulling down the ubuntu image, implicitly pulled down 4 new layers, a611792b4cac502995fa88a888261dfba0b5d852e72f9db9e075050991423779 d181f1a41fc35a45c16e8bfcb8eee6f768f3b98f82210a43ea65f284a45fcd65 dac2f37f6280a076836d39b87b0ae5ebf5c0d386b6d8b991b103aadbcebaa7c6 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709 The overlay2 storage driver in essence layers different directories on the host and presents them as a single directory. base layer or lowerdir, diff layer or upperdir, overlay layer (user view), and work dir. OverlayFS refers to the lower directories as lowerdir , which contains the base image and the read-only (R/O) layers that are pulled down. The upper directory is called upperdir and is the read-write (R/W) container layer. The unified view or overlay layer is called merged . Finally, a workdir is a required, which is an empty directory used by overlay for internal use. The overlay2 driver supports up to 128 lower OverlayFS layers. The l directory contains shortened layer identifiers as symbolic links. Cleanup, docker system prune -a clear","title":"Lab 3. Managing Data in Containers"},{"location":"lab-3/#lab-3-managing-data-in-containers","text":"","title":"Lab 3 - Managing Data in Containers"},{"location":"lab-3/#overview","text":"By default all files created inside a container are stored on a writable container layer. That means that: If the container no longer exists, the data is lost, The container's writable layer is tightly coupled to the host machine, and To manage the file system, you need a storage driver that provides a union file system, using the Linux kernel. This extra abstraction reduces performance compared to data volumes which write directly to the filesystem. Docker provides two options to store files in the host machine: volumes and bind mounts . If you're running Docker on Linux, you can also use a tmpfs mount , and with Docker on Windows you can also use a named pipe . Volumes are stored in the host filesystem that is managed by Docker. Bind mounts are stored anywhere on the host system. tmpfs mounts are stored in the host memory only. Originally, the --mount flag was used for Docker Swarm services and the --volume flag was used for standalone containers. From Docker 17.06 and higher, you can also use --mount for standalone containers and it is in general more explicit and verbose than --volume .","title":"Overview"},{"location":"lab-3/#volumes","text":"A data volume or volume is a directory that bypasses the Union File System of Docker. There are three types of volumes: anonymous volume, named volume, and host volume.","title":"Volumes"},{"location":"lab-3/#anonymous-volume","text":"Let's create an instance of a popular open source NoSQL database called CouchDB and use an anonymous volume to store the data files for the database. To run an instance of CouchDB, use the CouchDB image from Docker Hub at https://hub.docker.com/_/couchdb . The docs say that the default for CouchDB is to write the database files to disk on the host system using its own internal volume management . Run the following command, docker run -d -p 5984:5984 --name my-couchdb -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 CouchDB will create an anonymous volume and generated a hashed name. Check the volumes on your host system, $ docker volume ls DRIVER VOLUME NAME local f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50 Set an environment variable VOLUME with the value of the generated name, export VOLUME=f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50 And inspect the volume that was created, use the hash name that was generated for the volume, $ docker volume inspect $VOLUME [ { \"CreatedAt\": \"2020-09-24T14:10:07Z\", \"Driver\": \"local\", \"Labels\": null, \"Mountpoint\": \"/var/lib/docker/volumes/f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50/_data\", \"Name\": \"f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50\", \"Options\": null, \"Scope\": \"local\" } ] You see that Docker has created and manages a volume in the Docker host filesystem under /var/lib/docker/volumes/$VOLUME_NAME/_data . Note that this is not a path on the host machine, but a part of the Docker managed filesystem. Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' Stop the container and start the container again, docker stop my-couchdb docker start my-couchdb Retrieve the document in the database to test that the data was persisted, $ curl -X GET -u admin:passw0rd1 http://127.0.0.1:5984/mydb/_all_docs {\"total_rows\":1,\"offset\":0,\"rows\":[ {\"id\":\"1\",\"key\":\"1\",\"value\":{\"rev\":\"1-c09289617e06b96bc747fb1201fea7f1\"}} ]} $ curl -X GET -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 {\"_id\":\"1\",\"_rev\":\"1-c09289617e06b96bc747fb1201fea7f1\",\"msg\":\"hello world\"}","title":"Anonymous Volume"},{"location":"lab-3/#sharing-volumes","text":"You can share an anonymous volume with another container by using the --volumes-from option. Create a busybox container with an anonymous volume mounted to a directory /data in the container, and using shell commands, write a message to a log file. $ docker run -it --name busybox1 -v /data busybox sh / # echo \"hello from busybox1\" > /data/hi.log / # ls /data hi.log / # exit Make sure the container busybox1 is stopped but not removed. $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 437fb4a271c1 busybox \"sh\" 18 seconds ago Exited (0) 4 seconds ago busybox1 Then create a second busybox container named busybox2 using the --volumes-from option to share the volume created by busybox1 , $ docker run --rm -it --name busybox2 --volumes-from busybox1 busybox sh / # ls -al /data / # cat /data/hi.log hello from busybox1 / # exit Docker created the anynomous volume that you were able to share using the --volumes-from option, and created a new anonymous volume. $ docker volume ls DRIVER VOLUME NAME local 83a3275e889506f3e8ff12cd50f7d5b501c1ace95672334597f9a071df439493 local f4e6b9f9568eeb165a56b2946847035414f5f9c2cad9ff79f18e800277ae1ebd Cleanup the existing volumes and container. docker stop my-couchdb docker rm my-couchdb docker rm busybox1 docker volume rm $(docker volume ls -q) docker system prune -a clear","title":"Sharing Volumes"},{"location":"lab-3/#named-volume","text":"A named volume and anonymous volume are similar in that Docker manages where they are located. However, a named volume can be referenced by name when mounting it to a container directory. This is helpful if you want to share a volume across multiple containers. First, create a named volume , docker volume create my-couchdb-data-volume Verify the volume was created, $ docker volume ls DRIVER VOLUME NAME local my-couchdb-data-volume Now create the CouchDB container using the named volume , docker run -d -p 5984:5984 --name my-couchdb -v my-couchdb-data-volume:/opt/couchdb/data -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 Wait until the CouchDB container is running and the instance is available. Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' It now is easy to share the volume with another container. For instance, read the content of the volume using the busybox image, and share the my-couchdb-data-volume volume by mounting the volume to a directory in the busybox container. $ docker run --rm -it --name busybox -v my-couchdb-data-volume:/myvolume busybox sh / # ls -al /myvolume/ total 40 drwxr-xr-x 4 5984 5984 4096 Sep 24 17:11 . drwxr-xr-x 1 root root 4096 Sep 24 17:14 .. drwxr-xr-x 2 5984 5984 4096 Sep 24 17:11 .delete -rw-r--r-- 1 5984 5984 8388 Sep 24 17:11 _dbs.couch -rw-r--r-- 1 5984 5984 8385 Sep 24 17:11 _nodes.couch drwxr-xr-x 4 5984 5984 4096 Sep 24 17:11 shards / # exit You can check the Docker managed filesystem for volumes by running a busybox container with privileged permission and set the process id to host to inspect the host system, and browse to the Docker managed directories. docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/volumes total 28 -rw------- 1 root root 32768 Nov 10 15:54 metadata.db drwxr-xr-x 3 root root 4096 Nov 10 15:54 my-couchdb-data-volume / # exit Cleanup, docker stop my-couchdb docker rm my-couchdb docker volume rm my-couchdb-data-volume docker system prune -a docker volume prune clear","title":"Named Volume"},{"location":"lab-3/#host-volume","text":"When you want to access the volume directory easily from the host machine directly instead of using the Docker managed directories, you can create a host volume . Let's use a directory in the current working directory (indicated with the command pwd ) called data , or choose your own data directory on the host machine, e.g. /home/couchdb/data . We let docker create the $(pwd)/data directory if it does not exist yet. We mount the host volume inside the CouchDB container to the container directory /opt/couchdb/data , which is the default data directory for CouchDB. Run the following command, docker run -d -p 5984:5984 --name my-couchdb -v $(pwd)/data:/opt/couchdb/data -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1 Verify that a directory data was created, $ ls -al total 16 drwxrwsrwx 3 root users 4096 Sep 24 16:27 . drwxrwxr-x 1 root root 4096 Jul 16 20:04 .. drwxr-sr-x 3 5984 5984 4096 Sep 24 16:27 data and that CouchDB has created data files here, $ ls -al data total 32 drwxr-sr-x 3 5984 5984 4096 Sep 24 16:27 . drwxrwsrwx 3 root users 4096 Sep 24 16:27 .. -rw-r--r-- 1 5984 5984 4257 Sep 24 16:27 _dbs.couch drwxr-sr-x 2 5984 5984 4096 Sep 24 16:27 .delete -rw-r--r-- 1 5984 5984 8385 Sep 24 16:27 _nodes.couch Also check that now, no managed volume was created by docker, because we are now using a host volume . docker volume ls and docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/volumes total 24 -rw------- 1 root root 32768 Nov 10 16:00 metadata.db / # exit Create a new database mydb and insert a new document with a hello world message. curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}' Note that CouchDB created a folder shards , $ ls -al data total 40 drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 . drwxrwsrwx 3 root users 4096 Sep 24 16:49 .. -rw-r--r-- 1 5984 5984 8388 Sep 24 16:49 _dbs.couch drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 .delete -rw-r--r-- 1 5984 5984 8385 Sep 24 16:49 _nodes.couch drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 shards List the content of the shards directory, $ ls -al data/shards total 16 drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 . drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 .. drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 00000000-7fffffff drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 80000000-ffffffff and the first shard, $ ls -al data/shards/00000000-7fffffff/ total 20 drwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 . drwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 .. -rw-r--r-- 1 5984 5984 8346 Sep 24 16:49 mydb.1600966173.couch A shard is a horizontal partition of data in a database. Partitioning data into shards and distributing copies of each shard to different nodes in a cluster gives the data greater durability against node loss. CouchDB automatically shards databases and distributes the subsets of documents among nodes. Cleanup, docker stop my-couchdb docker rm my-couchdb sudo rm -rf $(pwd)/data docker system prune -a","title":"Host Volume"},{"location":"lab-3/#bind-mounts","text":"The mount syntax is recommended by Docker over the volume syntax. Bind mounts have limited functionality compared to volumes. A file or directory is referenced by its full path on the host machine when mounted into a container. Bind mounts rely on the host machine\u2019s filesystem having a specific directory structure available and you cannot use the Docker CLI to manage bind mounts. Note that bind mounts can change the host filesystem via processes running in a container. Instead of using the -v syntax with three fields separated by colon separator (:), the mount syntax is more verbose and uses multiple key-value pairs: type: bind, volume or tmpfs, source: path to the file or directory on host machine, destination: path in container, readonly, bind-propagation: rprivate, private, rshared, shared, rslave, slave, consistency: consistent, delegated, cached, mount. mkdir data docker run -it --name busybox --mount type=bind,source=\"$(pwd)\"/data,target=/data busybox sh / # echo \"hello busybox\" > /data/hi.txt / # exit cat data/hi.txt","title":"Bind Mounts"},{"location":"lab-3/#optional-overlayfs","text":"OverlayFS is a union mount filesystem implementation for Linux. To understand what a Docker volume is, it helps to understand how layers and the filesystem work in Docker. To start a container, Docker takes the read-only image and creates a new read-write layer on top. To view the layers as one, Docker uses a Union File System or OverlayFS (Overlay File System), specifically the overlay2 storage driver. To see Docker host managed files, you need access to the Docker process file system. Using the --privileged and --pid=host flags you can access the host's process ID namespace from inside a container like busybox . You can then browse to Docker's /var/lib/docker/overlay2 directory to see the downloaded layers that are managed by Docker. To view the current list of layers in Docker, $ docker run -it --privileged --pid = host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/overlay2 total 16 drwx------ 3 root root 4096 Sep 25 19:44 0e55ecaa4d17c353191e68022d9a17fde64fb5e9217b07b5c56eb4c74dad5b32 drwx------ 5 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d drwx------ 4 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d-init drwx------ 2 root root 4096 Sep 25 19:44 l / # exit Pull down the ubuntu image and check again, $ docker pull ubuntu Using default tag: latest latest: Pulling from library/ubuntu e6ca3592b144: Pull complete 534a5505201d: Pull complete 990916bd23bb: Pull complete Digest: sha256:cbcf86d7781dbb3a6aa2bcea25403f6b0b443e20b9959165cf52d2cc9608e4b9 Status: Downloaded newer image for ubuntu:latest $ docker run -it --privileged --pid = host busybox nsenter -t 1 -m -u -n -i sh / # ls -l /var/lib/docker/overlay2/ total 36 drwx------ 3 root root 4096 Sep 25 19:44 0e55ecaa4d17c353191e68022d9a17fde64fb5e9217b07b5c56eb4c74dad5b32 drwx------ 4 root root 4096 Sep 25 19:45 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d drwx------ 4 root root 4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d-init drwx------ 4 root root 4096 Sep 25 19:46 a611792b4cac502995fa88a888261dfba0b5d852e72f9db9e075050991423779 drwx------ 3 root root 4096 Sep 25 19:46 d181f1a41fc35a45c16e8bfcb8eee6f768f3b98f82210a43ea65f284a45fcd65 drwx------ 4 root root 4096 Sep 25 19:46 dac2f37f6280a076836d39b87b0ae5ebf5c0d386b6d8b991b103aadbcebaa7c6 drwx------ 5 root root 4096 Sep 25 19:47 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709 drwx------ 4 root root 4096 Sep 25 19:47 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709-init drwx------ 2 root root 4096 Sep 25 19:47 l / # exit You see that pulling down the ubuntu image, implicitly pulled down 4 new layers, a611792b4cac502995fa88a888261dfba0b5d852e72f9db9e075050991423779 d181f1a41fc35a45c16e8bfcb8eee6f768f3b98f82210a43ea65f284a45fcd65 dac2f37f6280a076836d39b87b0ae5ebf5c0d386b6d8b991b103aadbcebaa7c6 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709 The overlay2 storage driver in essence layers different directories on the host and presents them as a single directory. base layer or lowerdir, diff layer or upperdir, overlay layer (user view), and work dir. OverlayFS refers to the lower directories as lowerdir , which contains the base image and the read-only (R/O) layers that are pulled down. The upper directory is called upperdir and is the read-write (R/W) container layer. The unified view or overlay layer is called merged . Finally, a workdir is a required, which is an empty directory used by overlay for internal use. The overlay2 driver supports up to 128 lower OverlayFS layers. The l directory contains shortened layer identifiers as symbolic links. Cleanup, docker system prune -a clear","title":"[Optional] OverlayFS"},{"location":"lab-x/","text":"Lab 3 - Introduction to Orchestration \u00b6 Overview \u00b6 So far you have learned how to run applications using docker on your local machine, but what about running dockerized applications in production? There are a number of problems that come with building an application for production: scheduling services across distributed nodes, maintaining high availability, implementing reconciliation, scaling, and logging... just to name a few. There are several orchestration solutions out there that help you solve some of these problems. The most widely adopted orchestration platform is Kubernetes. The IBM Kubernetes Service is a managed service of Kubernetes to run containers in production. Red Hat Openshift is an extension of Kubernetes with Enterprise grade services to provide a more secure and integrated extension of Kubernetes. Red Hat OpenShift Kubernetes Service (ROKS) on IBM Cloud is a managed service of OpenShift. You can also create a free 1-month, 1 node OpenShift cluster on OpenShift Online . This lab is for those who are interested in learning how to orchestrate applications using Docker Swarm. Docker Swarm is the orchestration tool that comes built-in to the Docker Engine. We will be using a few Docker commands in this lab. For full documentation on available commands check out the Docker official documentation . Prerequisites \u00b6 In order to complete a lab about orchestrating an application that is deployed across multiple hosts, you need... well, multiple hosts. Therefor, for this lab you will be using the multi-node support provided by Play with Docker . This is the easiest way to test out Docker Swarm, without having to deal with installing docker on multiple hosts yourself. Step 1: Create your first swarm \u00b6 In this step, we will create our first swarm using play-with-docker. Navigate to Play with Docker Click \"add new instance\" on the lefthand side three times to create three nodes Our first swarm cluster will have three nodes. Initialize the swarm on node 1 $ docker swarm init --advertise-addr eth0 Swarm initialized: current node ( vq7xx5j4dpe04rgwwm5ur63ce ) is now a manager. To add a worker to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-50qba7hmo5exuapkmrj6jki8knfvinceo68xjmh322y7c8f0pj-87mjqjho30uue43oqbhhthjui \\ 10 .0.120.3:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. You can think of docker swarm as a special \"mode\" that is activated by the command: docker swarm init . The --advertise-addr specifies the address in which the other nodes will use to join the swarm. This docker swarm init command generates a join token. The token makes sure that no malicious nodes join our swarm. We will need to use this token to join the other nodes to the swarm. For convenience, the output includes the full command docker swarm join which you can just copy/paste to the other nodes. On both node2 and node3, copy and run the docker swarm join command that was outputted to YOUR console by the last command. You now have a three node swarm! Back on node1, run docker node ls to verify your 3 node cluster. $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 7x9s8baa79l29zdsx95i1tfjp node3 Ready Active x223z25t7y7o4np3uq45d49br node2 Ready Active zdqbsoxa6x1bubg3jyjdmrnrn * node1 Ready Active Leader This command outputs the three nodes in our swarm. The * next to the ID of the node represents the node that handled that specific command ( docker node ls in this case). Our node consists of 1 manager node and 2 workers nodes. Managers handle commands and manage the state of the swarm. Workers cannot handle commands and are simply used to run containers at scale. By default, managers are also used to run containers. All docker service commands for the rest of this lab need to be executed on the manager node (Node1). Note: While we will control the Swarm directly from the node in which its running, you can control a docker swarm remotely by connecting to the docker engine of the manager via the remote API or activating a remote host from your local docker installation (using the $DOCKER_HOST and $DOCKER_CERT_PATH environment variables). This will become useful when you want to control production applications remotely instead of ssh-ing directly into production servers. Step 2: Deploy your first service \u00b6 Now that we have our 3 node Swarm cluster initialized, let's deploy some containers. To run containers on a Docker Swarm, we want to create a service. A service is an abstraction that represents multiple containers of the same image deployed across a distributed cluster. Let's do a simple example using Nginx. For now we will create a service with just 1 running container, but we will scale up later. Deploy a service using Nginx $ docker service create --detach = true --name nginx1 --publish 80 :80 --mount source = /etc/hostname,target = /usr/share/nginx/html/index.html,type = bind,ro nginx:1.18 pgqdxr41dpy8qwkn6qm7vke0q This above statement is declarative , and docker swarm will actively try to maintain the state declared in this command unless explicitly changed via another docker service command. This behavior comes in handy when nodes go down, for example, and containers are automatically rescheduled on other nodes. We will see a demonstration of that a little later on in this lab. The --mount flag is a neat trick to have nginx print out the hostname of the node it's running on. This will come in handy later in this lab when we start load balancing between multiple containers of nginx that are distributed across different nodes in the cluster, and we want to see which node in the swarm is serving the request. We are using nginx tag \"1.18\" in this command. We will demonstrate a rolling update with version 1.19 later in this lab. The --publish command makes use of the swarm's built in routing mesh . In this case port 80 is exposed on every node in the swarm . The routing mesh will route a request coming in on port 80 to one of the nodes running the container. Inspect the service You can use docker service ls to inspect the service you just created. $ docker service ls ID NAME MODE REPLICAS IMAGE PORTS pgqdxr41dpy8 nginx1 replicated 1 /1 nginx:1.18 *:80->80/tcp Check out the running container of the service To take a deeper look at the running tasks, you can use docker service ps . A task is yet another abstraction using in docker swarm that represents the running instances of a service. In this case, there is a 1-1 mapping between a task and a container. $ docker service ps nginx1 ID NAME IMAGE NODE DESIRED STATE CURRENT STAT E ERROR PORTS iu3ksewv7qf9 nginx1.1 nginx:1.18 node1 Running Running 8 minutes ago If you happen to know which node your container is running on (you can see which node based on the output from docker service ps ), you can use docker container ls to see the container running on that specific node. Test the service Because of the routing mesh, we can send a request to any node of the swarm on port 80. This request will be automatically routed to the one node that is running our nginx container. Try this on each node: $ curl localhost:80 node1 Curling will output the hostname where the container is running. For this example, it is running on \"node1\", but yours might be different. Step 3: Scale your service \u00b6 In production we may need to handle large amounts of traffic to our application. So let's scale! Update your service with an updated number of replicas We are going to use the docker service command to update the nginx service we created earlier to include 5 replicas. This is defining a new state for our service. $ docker service update --replicas = 5 --detach = true nginx1 nginx1 As soon as this command is run the following happens: The state of the service is updated to 5 replicas (which is stored in the swarms internal storage). Docker swarm recognizes that the number of replicas that is scheduled now does not match the declared state of 5. Docker swarm schedules 5 more tasks (containers) in an attempt to meet the declared state for the service. This swarm is actively checking to see if the desired state is equal to actual state, and will attempt to reconcile if needed. Check the running instances After a few seconds, you should see that the swarm did its job, and successfully started 9 more containers. Notice that the containers are scheduled across all three nodes of the cluster. The default placement strategy that is used to decide where new containers are to be run is \"emptiest node\", but that can be changed based on your need. $ docker service ps nginx1 ID NAME IMAGE NODE DESIRED STATE CURRENT STAT E ERROR PORTS iu3ksewv7qf9 nginx1.1 nginx:1.18 node1 Running Running 17 m inutes ago lfz1bhl6v77r nginx1.2 nginx:1.18 node2 Running Running 6 mi nutes ago qururb043dwh nginx1.3 nginx:1.18 node3 Running Running 6 mi nutes ago q53jgeeq7y1x nginx1.4 nginx:1.18 node3 Running Running 6 mi nutes ago xj271k2829uz nginx1.5 nginx:1.18 node1 Running Running 7 mi nutes ago Send a bunch of requests to localhost:80 The --publish 80:80 is still in effect for this service, that was not changed when we ran docker service update . However, now when we send requests on port 80, the routing mesh has multiple containers in which to route requests to. The routing mesh acts as a load balancer for these containers, alternating where it routes requests to. Let's try it out by curling multiple times. Note, that it doesn't matter which node you send the requests. There is no connection between the node that receives the request, and the node that that request is routed to. $ curl localhost:80 node3 $ curl localhost:80 node3 $ curl localhost:80 node2 $ curl localhost:80 node1 $ curl localhost:80 node1 You should see which node is serving each request because of the nifty --mount command we used earlier. Limits of the routing Mesh The routing mesh can only publish one service on port 80. If you want multiple services exposed on port 80, then you can use an external application load balancer outside of the swarm to accomplish this. Check the aggregated logs for the service Another easy way to see which nodes those requests were routed to is to check the aggregated logs. We can get aggregated logs for the service using docker service logs [service name] . This aggregates the output from every running container, i.e. the output from docker container logs [container name] . $ docker service logs nginx1 nginx1.4.q53jgeeq7y1x@node3 | 10 .255.0.2 - - [ 28 /Jun/2017:18:59:39 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" nginx1.2.lfz1bhl6v77r@node2 | 10 .255.0.2 - - [ 28 /Jun/2017:18:59:40 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" nginx1.5.xj271k2829uz@node1 | 10 .255.0.2 - - [ 28 /Jun/2017:18:59:41 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" nginx1.1.iu3ksewv7qf9@node1 | 10 .255.0.2 - - [ 28 /Jun/2017:18:50:23 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" nginx1.1.iu3ksewv7qf9@node1 | 10 .255.0.2 - - [ 28 /Jun/2017:18:59:41 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" nginx1.3.qururb043dwh@node3 | 10 .255.0.2 - - [ 28 /Jun/2017:18:59:38 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" Based on these logs we can see that each request was served by a different container. In addition to seeing whether the request was sent to node1, node2, or node3, you can also see which container on each node that it was sent to. For example nginx1.5 means that request was sent to container with that same name as indicated in the output of docker service ps nginx1 . Step 4: Rolling Updates \u00b6 Now that we have our service deployed, let's demonstrate a release of our application. We are going to update the version of Nginx to version \"1.19\". To do this update we are going to use the docker service update command. $ docker service update --image nginx:1.19 --detach = true nginx1 1a2b3c This will trigger a rolling update of the swarm. Quickly type in docker service ps nginx1 over and over to see the updates in real time. You can fine tune the rolling update using these options: --update-parallelism will dictate the number of containers to update at once. (defaults to 1) --update-delay will dictate the delay between finishing updating a set of containers before moving on to the next set. After a few seconds, run docker service ps nginx1 to see all the images have been updated to nginx:1.19. $ docker service ps nginx1 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS l4s05d18j9ga nginx1.1 nginx:1.19 node1 Ready Ready 1 second ago zsm6as2ffq8g \\_ nginx1.1 nginx:1.18 node1 Shutdown Running 1 second ago nk9awj3zercp nginx1.2 nginx:1.19 node2 Running Running 15 seconds ago 21gxmjea135j \\_ nginx1.2 nginx:1.18 node3 Shutdown Shutdown 16 seconds ago 2cdadovtek2m nginx1.3 nginx:1.19 node1 Running Running 6 seconds ago ld0zgx5et2xy \\_ nginx1.3 nginx:1.18 node1 Shutdown Shutdown 6 seconds ago p3gu8wcpmeax nginx1.4 nginx:1.19 node3 Running Running 1 second ago wo4ioe8lrju6 \\_ nginx1.4 nginx:1.18 node2 Shutdown Shutdown 2 seconds ago lpe6dkkr4osc nginx1.5 nginx:1.19 node3 Running Running 10 seconds ago 6qvqfb6x77u4 \\_ nginx1.5 nginx:1.18 node3 Shutdown Shutdown 11 seconds ago Repeat the above command, until all images have successfully updated to the latest version of nginx! Step 5: Reconciliation \u00b6 In the previous step, we updated the state of our service using docker service update . We saw Docker Swarm in action as it recognized the mismatch between desired state and actual state, and attempted to solve this issue. The \"inspect->adapt\" model of docker swarm enables it to perform reconciliation when something goes wrong. For example, when a node in the swarm goes down it might take down running containers with it. The swarm will recognize this loss of containers, and will attempt to reschedule containers on available nodes in order to achieve the desired state for that service. We are going to remove a node, and see tasks of our nginx1 service be rescheduled on other nodes automatically. For the sake of clean output, first create a brand new service by copying the line below. We will change the name, and the publish port to avoid conflicts with our existing service. We will also add the --replicas command to scale the service with 5 instances. $ docker service create --detach = true --name nginx2 --replicas = 5 --publish 81 :80 --mount source = /etc/hostname,target = /usr/share/nginx/html/index.html,type = bind,ro nginx:1.18 aiqdh5n9fyacgvb2g82s412js On Node1, use watch to watch the update from the output of docker service ps . Note \"watch\" is a linux utility and might not be available on other platforms. $ watch -n 1 docker service ps nginx2 ok This should result in a window that looks like this: Every 1 .0s: docker service ps nginx2 2020 -09-23 20 :59:43 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS nveflkbbzhia nginx2.1 nginx:1.18 node1 Running Running 41 seconds ago qlk6avfcjqft nginx2.2 nginx:1.18 node2 Running Running 41 seconds ago psizpiwxt1ta nginx2.3 nginx:1.18 node3 Running Running 41 seconds ago 0kmrqeneqztk nginx2.4 nginx:1.18 node2 Running Running 41 seconds ago zdg9j4aiqt8w nginx2.5 nginx:1.18 node3 Running Running 41 seconds ago Click on Node3, and type the command to leave the swarm cluster. $ docker swarm leave ok This is the \"nice\" way to leave the swarm, but you can also kill the node and the following behavior will be the same. Click on Node1 to watch the reconciliation in action. You should see that the swarm will attempt to get back to the declared state by rescheduling the containers that were running on node3 to node1 and node2 automatically. $ docker service ps nginx2 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS jeq4604k1v9k nginx2.1 nginx:1.18 node1 Running Running 5 seconds ago 6koehbhsfbi7 \\_ nginx2.1 nginx:1.18 node3 Shutdown Running 21 seconds ago dou2brjfr6lt nginx2.2 nginx:1.18 node1 Running Running 26 seconds ago 8jc41tgwowph nginx2.3 nginx:1.18 node2 Running Running 27 seconds ago n5n8zryzg6g6 nginx2.4 nginx:1.18 node1 Running Running 26 seconds ago cnofhk1v5bd8 nginx2.5 nginx:1.18 node2 Running Running 27 seconds ago [ node1 ] ( loc Number of nodes \u00b6 In this lab, our Docker Swarm cluster consists of one master, and two worker nodes. This configuration is not highly available. The manager node contains the necessary information to manage the cluster, so if this node goes down, the cluster will cease to function. For a production application, you will want to provision a cluster with multiple manager nodes to allow for manager node failures. For manager nodes you want at least 3, but typically no more than 7. Managers implement the raft consensus algorithm, which requires that more than 50% of the nodes agree on the state that is being stored for the cluster. If you don't achieve >50%, the swarm will cease to operate correctly. For this reason, the following can be assumed about node failure tolerance. 3 manager nodes tolerates 1 node failure 5 manager nodes tolerates 2 node failures 7 manager nodes tolerates 3 node failures It is possible to have an even number of manager nodes, but it adds no value in terms of the number of node failures. For example, 4 manager nodes would only tolerate 1 node failure, which is the same tolerance as a 3 manager node cluster. The more manager nodes you have, the harder it is to achieve a consensus on the state of a cluster. While you typically want to limit the number of manager nodes to no more than 7, you can scale the number of worker nodes much higher than that. Worker nodes can scale up into the 1000's of nodes. Worker nodes communicate using the gossip protocol, which is optimized to be highly performant under large traffic and a large number of nodes. If you are using Play with Docker , you can easily deploy multiple manager node clusters using the built in templates. Click the templates icon in the upper left to see what templates are available. Summary \u00b6 In this lab, you got an introduction to problems that come with running container with production such as scheduling services across distributed nodes, maintaining high availability, implementing reconciliation, scaling, and logging. We used the orchestration tool that comes built-in to the Docker Engine- Docker Swarm, to address some of these issues. Key Takeaways: The Docker Swarm schedules services using a declarative language. You declare the state, and the swarm attempts to maintain and reconcile to make sure the actual state == desired state Docker Swarm is composed of manager and worker nodes. Only managers can maintain the state of the swarm and accept commands to modify it. Workers have high scability and are only used to run containers. By default managers can run containers as well. The routing mesh built into swarm means that any port that is published at the service level will be exposed on every node in the swarm. Requests to a published service port will be routed automatically to a container of the service that is running in the swarm. Many tools out there exist to help solve problems with orchestration containerized applications in production, include Docker Swarm, and the IBM Kubernetes Service .","title":"Lab 3 - Introduction to Orchestration"},{"location":"lab-x/#lab-3-introduction-to-orchestration","text":"","title":"Lab 3 - Introduction to Orchestration"},{"location":"lab-x/#overview","text":"So far you have learned how to run applications using docker on your local machine, but what about running dockerized applications in production? There are a number of problems that come with building an application for production: scheduling services across distributed nodes, maintaining high availability, implementing reconciliation, scaling, and logging... just to name a few. There are several orchestration solutions out there that help you solve some of these problems. The most widely adopted orchestration platform is Kubernetes. The IBM Kubernetes Service is a managed service of Kubernetes to run containers in production. Red Hat Openshift is an extension of Kubernetes with Enterprise grade services to provide a more secure and integrated extension of Kubernetes. Red Hat OpenShift Kubernetes Service (ROKS) on IBM Cloud is a managed service of OpenShift. You can also create a free 1-month, 1 node OpenShift cluster on OpenShift Online . This lab is for those who are interested in learning how to orchestrate applications using Docker Swarm. Docker Swarm is the orchestration tool that comes built-in to the Docker Engine. We will be using a few Docker commands in this lab. For full documentation on available commands check out the Docker official documentation .","title":"Overview"},{"location":"lab-x/#prerequisites","text":"In order to complete a lab about orchestrating an application that is deployed across multiple hosts, you need... well, multiple hosts. Therefor, for this lab you will be using the multi-node support provided by Play with Docker . This is the easiest way to test out Docker Swarm, without having to deal with installing docker on multiple hosts yourself.","title":"Prerequisites"},{"location":"lab-x/#step-1-create-your-first-swarm","text":"In this step, we will create our first swarm using play-with-docker. Navigate to Play with Docker Click \"add new instance\" on the lefthand side three times to create three nodes Our first swarm cluster will have three nodes. Initialize the swarm on node 1 $ docker swarm init --advertise-addr eth0 Swarm initialized: current node ( vq7xx5j4dpe04rgwwm5ur63ce ) is now a manager. To add a worker to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-50qba7hmo5exuapkmrj6jki8knfvinceo68xjmh322y7c8f0pj-87mjqjho30uue43oqbhhthjui \\ 10 .0.120.3:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. You can think of docker swarm as a special \"mode\" that is activated by the command: docker swarm init . The --advertise-addr specifies the address in which the other nodes will use to join the swarm. This docker swarm init command generates a join token. The token makes sure that no malicious nodes join our swarm. We will need to use this token to join the other nodes to the swarm. For convenience, the output includes the full command docker swarm join which you can just copy/paste to the other nodes. On both node2 and node3, copy and run the docker swarm join command that was outputted to YOUR console by the last command. You now have a three node swarm! Back on node1, run docker node ls to verify your 3 node cluster. $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 7x9s8baa79l29zdsx95i1tfjp node3 Ready Active x223z25t7y7o4np3uq45d49br node2 Ready Active zdqbsoxa6x1bubg3jyjdmrnrn * node1 Ready Active Leader This command outputs the three nodes in our swarm. The * next to the ID of the node represents the node that handled that specific command ( docker node ls in this case). Our node consists of 1 manager node and 2 workers nodes. Managers handle commands and manage the state of the swarm. Workers cannot handle commands and are simply used to run containers at scale. By default, managers are also used to run containers. All docker service commands for the rest of this lab need to be executed on the manager node (Node1). Note: While we will control the Swarm directly from the node in which its running, you can control a docker swarm remotely by connecting to the docker engine of the manager via the remote API or activating a remote host from your local docker installation (using the $DOCKER_HOST and $DOCKER_CERT_PATH environment variables). This will become useful when you want to control production applications remotely instead of ssh-ing directly into production servers.","title":"Step 1: Create your first swarm"},{"location":"lab-x/#step-2-deploy-your-first-service","text":"Now that we have our 3 node Swarm cluster initialized, let's deploy some containers. To run containers on a Docker Swarm, we want to create a service. A service is an abstraction that represents multiple containers of the same image deployed across a distributed cluster. Let's do a simple example using Nginx. For now we will create a service with just 1 running container, but we will scale up later. Deploy a service using Nginx $ docker service create --detach = true --name nginx1 --publish 80 :80 --mount source = /etc/hostname,target = /usr/share/nginx/html/index.html,type = bind,ro nginx:1.18 pgqdxr41dpy8qwkn6qm7vke0q This above statement is declarative , and docker swarm will actively try to maintain the state declared in this command unless explicitly changed via another docker service command. This behavior comes in handy when nodes go down, for example, and containers are automatically rescheduled on other nodes. We will see a demonstration of that a little later on in this lab. The --mount flag is a neat trick to have nginx print out the hostname of the node it's running on. This will come in handy later in this lab when we start load balancing between multiple containers of nginx that are distributed across different nodes in the cluster, and we want to see which node in the swarm is serving the request. We are using nginx tag \"1.18\" in this command. We will demonstrate a rolling update with version 1.19 later in this lab. The --publish command makes use of the swarm's built in routing mesh . In this case port 80 is exposed on every node in the swarm . The routing mesh will route a request coming in on port 80 to one of the nodes running the container. Inspect the service You can use docker service ls to inspect the service you just created. $ docker service ls ID NAME MODE REPLICAS IMAGE PORTS pgqdxr41dpy8 nginx1 replicated 1 /1 nginx:1.18 *:80->80/tcp Check out the running container of the service To take a deeper look at the running tasks, you can use docker service ps . A task is yet another abstraction using in docker swarm that represents the running instances of a service. In this case, there is a 1-1 mapping between a task and a container. $ docker service ps nginx1 ID NAME IMAGE NODE DESIRED STATE CURRENT STAT E ERROR PORTS iu3ksewv7qf9 nginx1.1 nginx:1.18 node1 Running Running 8 minutes ago If you happen to know which node your container is running on (you can see which node based on the output from docker service ps ), you can use docker container ls to see the container running on that specific node. Test the service Because of the routing mesh, we can send a request to any node of the swarm on port 80. This request will be automatically routed to the one node that is running our nginx container. Try this on each node: $ curl localhost:80 node1 Curling will output the hostname where the container is running. For this example, it is running on \"node1\", but yours might be different.","title":"Step 2: Deploy your first service"},{"location":"lab-x/#step-3-scale-your-service","text":"In production we may need to handle large amounts of traffic to our application. So let's scale! Update your service with an updated number of replicas We are going to use the docker service command to update the nginx service we created earlier to include 5 replicas. This is defining a new state for our service. $ docker service update --replicas = 5 --detach = true nginx1 nginx1 As soon as this command is run the following happens: The state of the service is updated to 5 replicas (which is stored in the swarms internal storage). Docker swarm recognizes that the number of replicas that is scheduled now does not match the declared state of 5. Docker swarm schedules 5 more tasks (containers) in an attempt to meet the declared state for the service. This swarm is actively checking to see if the desired state is equal to actual state, and will attempt to reconcile if needed. Check the running instances After a few seconds, you should see that the swarm did its job, and successfully started 9 more containers. Notice that the containers are scheduled across all three nodes of the cluster. The default placement strategy that is used to decide where new containers are to be run is \"emptiest node\", but that can be changed based on your need. $ docker service ps nginx1 ID NAME IMAGE NODE DESIRED STATE CURRENT STAT E ERROR PORTS iu3ksewv7qf9 nginx1.1 nginx:1.18 node1 Running Running 17 m inutes ago lfz1bhl6v77r nginx1.2 nginx:1.18 node2 Running Running 6 mi nutes ago qururb043dwh nginx1.3 nginx:1.18 node3 Running Running 6 mi nutes ago q53jgeeq7y1x nginx1.4 nginx:1.18 node3 Running Running 6 mi nutes ago xj271k2829uz nginx1.5 nginx:1.18 node1 Running Running 7 mi nutes ago Send a bunch of requests to localhost:80 The --publish 80:80 is still in effect for this service, that was not changed when we ran docker service update . However, now when we send requests on port 80, the routing mesh has multiple containers in which to route requests to. The routing mesh acts as a load balancer for these containers, alternating where it routes requests to. Let's try it out by curling multiple times. Note, that it doesn't matter which node you send the requests. There is no connection between the node that receives the request, and the node that that request is routed to. $ curl localhost:80 node3 $ curl localhost:80 node3 $ curl localhost:80 node2 $ curl localhost:80 node1 $ curl localhost:80 node1 You should see which node is serving each request because of the nifty --mount command we used earlier. Limits of the routing Mesh The routing mesh can only publish one service on port 80. If you want multiple services exposed on port 80, then you can use an external application load balancer outside of the swarm to accomplish this. Check the aggregated logs for the service Another easy way to see which nodes those requests were routed to is to check the aggregated logs. We can get aggregated logs for the service using docker service logs [service name] . This aggregates the output from every running container, i.e. the output from docker container logs [container name] . $ docker service logs nginx1 nginx1.4.q53jgeeq7y1x@node3 | 10 .255.0.2 - - [ 28 /Jun/2017:18:59:39 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" nginx1.2.lfz1bhl6v77r@node2 | 10 .255.0.2 - - [ 28 /Jun/2017:18:59:40 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" nginx1.5.xj271k2829uz@node1 | 10 .255.0.2 - - [ 28 /Jun/2017:18:59:41 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" nginx1.1.iu3ksewv7qf9@node1 | 10 .255.0.2 - - [ 28 /Jun/2017:18:50:23 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" nginx1.1.iu3ksewv7qf9@node1 | 10 .255.0.2 - - [ 28 /Jun/2017:18:59:41 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" nginx1.3.qururb043dwh@node3 | 10 .255.0.2 - - [ 28 /Jun/2017:18:59:38 +0000 ] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7. 52.1\" \"-\" Based on these logs we can see that each request was served by a different container. In addition to seeing whether the request was sent to node1, node2, or node3, you can also see which container on each node that it was sent to. For example nginx1.5 means that request was sent to container with that same name as indicated in the output of docker service ps nginx1 .","title":"Step 3: Scale your service"},{"location":"lab-x/#step-4-rolling-updates","text":"Now that we have our service deployed, let's demonstrate a release of our application. We are going to update the version of Nginx to version \"1.19\". To do this update we are going to use the docker service update command. $ docker service update --image nginx:1.19 --detach = true nginx1 1a2b3c This will trigger a rolling update of the swarm. Quickly type in docker service ps nginx1 over and over to see the updates in real time. You can fine tune the rolling update using these options: --update-parallelism will dictate the number of containers to update at once. (defaults to 1) --update-delay will dictate the delay between finishing updating a set of containers before moving on to the next set. After a few seconds, run docker service ps nginx1 to see all the images have been updated to nginx:1.19. $ docker service ps nginx1 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS l4s05d18j9ga nginx1.1 nginx:1.19 node1 Ready Ready 1 second ago zsm6as2ffq8g \\_ nginx1.1 nginx:1.18 node1 Shutdown Running 1 second ago nk9awj3zercp nginx1.2 nginx:1.19 node2 Running Running 15 seconds ago 21gxmjea135j \\_ nginx1.2 nginx:1.18 node3 Shutdown Shutdown 16 seconds ago 2cdadovtek2m nginx1.3 nginx:1.19 node1 Running Running 6 seconds ago ld0zgx5et2xy \\_ nginx1.3 nginx:1.18 node1 Shutdown Shutdown 6 seconds ago p3gu8wcpmeax nginx1.4 nginx:1.19 node3 Running Running 1 second ago wo4ioe8lrju6 \\_ nginx1.4 nginx:1.18 node2 Shutdown Shutdown 2 seconds ago lpe6dkkr4osc nginx1.5 nginx:1.19 node3 Running Running 10 seconds ago 6qvqfb6x77u4 \\_ nginx1.5 nginx:1.18 node3 Shutdown Shutdown 11 seconds ago Repeat the above command, until all images have successfully updated to the latest version of nginx!","title":"Step 4: Rolling Updates"},{"location":"lab-x/#step-5-reconciliation","text":"In the previous step, we updated the state of our service using docker service update . We saw Docker Swarm in action as it recognized the mismatch between desired state and actual state, and attempted to solve this issue. The \"inspect->adapt\" model of docker swarm enables it to perform reconciliation when something goes wrong. For example, when a node in the swarm goes down it might take down running containers with it. The swarm will recognize this loss of containers, and will attempt to reschedule containers on available nodes in order to achieve the desired state for that service. We are going to remove a node, and see tasks of our nginx1 service be rescheduled on other nodes automatically. For the sake of clean output, first create a brand new service by copying the line below. We will change the name, and the publish port to avoid conflicts with our existing service. We will also add the --replicas command to scale the service with 5 instances. $ docker service create --detach = true --name nginx2 --replicas = 5 --publish 81 :80 --mount source = /etc/hostname,target = /usr/share/nginx/html/index.html,type = bind,ro nginx:1.18 aiqdh5n9fyacgvb2g82s412js On Node1, use watch to watch the update from the output of docker service ps . Note \"watch\" is a linux utility and might not be available on other platforms. $ watch -n 1 docker service ps nginx2 ok This should result in a window that looks like this: Every 1 .0s: docker service ps nginx2 2020 -09-23 20 :59:43 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS nveflkbbzhia nginx2.1 nginx:1.18 node1 Running Running 41 seconds ago qlk6avfcjqft nginx2.2 nginx:1.18 node2 Running Running 41 seconds ago psizpiwxt1ta nginx2.3 nginx:1.18 node3 Running Running 41 seconds ago 0kmrqeneqztk nginx2.4 nginx:1.18 node2 Running Running 41 seconds ago zdg9j4aiqt8w nginx2.5 nginx:1.18 node3 Running Running 41 seconds ago Click on Node3, and type the command to leave the swarm cluster. $ docker swarm leave ok This is the \"nice\" way to leave the swarm, but you can also kill the node and the following behavior will be the same. Click on Node1 to watch the reconciliation in action. You should see that the swarm will attempt to get back to the declared state by rescheduling the containers that were running on node3 to node1 and node2 automatically. $ docker service ps nginx2 ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS jeq4604k1v9k nginx2.1 nginx:1.18 node1 Running Running 5 seconds ago 6koehbhsfbi7 \\_ nginx2.1 nginx:1.18 node3 Shutdown Running 21 seconds ago dou2brjfr6lt nginx2.2 nginx:1.18 node1 Running Running 26 seconds ago 8jc41tgwowph nginx2.3 nginx:1.18 node2 Running Running 27 seconds ago n5n8zryzg6g6 nginx2.4 nginx:1.18 node1 Running Running 26 seconds ago cnofhk1v5bd8 nginx2.5 nginx:1.18 node2 Running Running 27 seconds ago [ node1 ] ( loc","title":"Step 5: Reconciliation"},{"location":"lab-x/#number-of-nodes","text":"In this lab, our Docker Swarm cluster consists of one master, and two worker nodes. This configuration is not highly available. The manager node contains the necessary information to manage the cluster, so if this node goes down, the cluster will cease to function. For a production application, you will want to provision a cluster with multiple manager nodes to allow for manager node failures. For manager nodes you want at least 3, but typically no more than 7. Managers implement the raft consensus algorithm, which requires that more than 50% of the nodes agree on the state that is being stored for the cluster. If you don't achieve >50%, the swarm will cease to operate correctly. For this reason, the following can be assumed about node failure tolerance. 3 manager nodes tolerates 1 node failure 5 manager nodes tolerates 2 node failures 7 manager nodes tolerates 3 node failures It is possible to have an even number of manager nodes, but it adds no value in terms of the number of node failures. For example, 4 manager nodes would only tolerate 1 node failure, which is the same tolerance as a 3 manager node cluster. The more manager nodes you have, the harder it is to achieve a consensus on the state of a cluster. While you typically want to limit the number of manager nodes to no more than 7, you can scale the number of worker nodes much higher than that. Worker nodes can scale up into the 1000's of nodes. Worker nodes communicate using the gossip protocol, which is optimized to be highly performant under large traffic and a large number of nodes. If you are using Play with Docker , you can easily deploy multiple manager node clusters using the built in templates. Click the templates icon in the upper left to see what templates are available.","title":"Number of nodes"},{"location":"lab-x/#summary","text":"In this lab, you got an introduction to problems that come with running container with production such as scheduling services across distributed nodes, maintaining high availability, implementing reconciliation, scaling, and logging. We used the orchestration tool that comes built-in to the Docker Engine- Docker Swarm, to address some of these issues. Key Takeaways: The Docker Swarm schedules services using a declarative language. You declare the state, and the swarm attempts to maintain and reconcile to make sure the actual state == desired state Docker Swarm is composed of manager and worker nodes. Only managers can maintain the state of the swarm and accept commands to modify it. Workers have high scability and are only used to run containers. By default managers can run containers as well. The routing mesh built into swarm means that any port that is published at the service level will be exposed on every node in the swarm. Requests to a published service port will be routed automatically to a container of the service that is running in the swarm. Many tools out there exist to help solve problems with orchestration containerized applications in production, include Docker Swarm, and the IBM Kubernetes Service .","title":"Summary"}]}